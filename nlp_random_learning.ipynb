{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Random Saving"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoder_c_optimizer : optim.Adadelta = optim.Adadelta(encoder_c.parameters(), lr= learning_rate)\n",
    "encoder_a_optimizer : optim.Adadelta = optim.Adadelta(encoder_a.parameters(), lr= learning_rate)\n",
    "decoder_optimizer :  optim.Adadelta = optim.Adadelta(decoder.parameters(), lr= learning_rate)\n",
    "\n",
    "data_pairs = train_data\n",
    "\n",
    "training_pairs = [tensorsFromPair(random.choice(data_pairs)) for i in range(10)]\n",
    "training_pair = training_pairs[5 - 1]\n",
    "input_tensor_c, input_tensor_a, target_tensor = training_pair[0], training_pair[1], training_pair[2]\n",
    "\n",
    "max_length = 15\n",
    "\n",
    "input_c_tensor, input_a_tensor, target_tensor = input_tensor_c, input_tensor_a, target_tensor\n",
    "\n",
    "encoder_c_hidden = encoder_c.initHidden()\n",
    "encoder_a_hidden = encoder_a.initHidden()\n",
    "\n",
    "# zero grad\n",
    "encoder_a_optimizer.zero_grad()\n",
    "encoder_c_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "\n",
    "# len of sentence of c, a, t, that already; e.g input_c_length= len(input_c_tensor)\n",
    "input_c_length = input_c_tensor.size(0)\n",
    "input_a_length = input_a_tensor.size(0)\n",
    "target_length = target_tensor.size(0)\n",
    "\n",
    "encoder_c_outputs = torch.zeros(max_length, encoder_c.hidden_size, device=device)\n",
    "encoder_a_outputs = torch.zeros(max_length, encoder_a.hidden_size, device=device)\n",
    "\n",
    "loss = 0\n",
    "\n",
    "for ei in range(input_c_length):\n",
    "    encoder_c_output, encoder_c_hidden = encoder_c(input_c_tensor[ei], encoder_c_hidden)\n",
    "    encoder_c_outputs[ei] = encoder_c_output[0, 0]\n",
    "\n",
    "for ei in range(input_a_length):\n",
    "    encoder_a_output, encoder_a_hidden = encoder_a(input_a_tensor[ei], encoder_a_hidden)\n",
    "    encoder_a_outputs[ei] = encoder_a_output[0, 0]\n",
    "\n",
    "decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "decoder_hidden = torch.cat((encoder_c_hidden, encoder_a_hidden), 2)\n",
    "\n",
    "use_teacher_forcing = True\n",
    "\n",
    "decoder_output_2 = None\n",
    "\n",
    "\n",
    "aggregate_outputs = torch.zeros(target_length,OUTPUT_DIM ,device=device)\n",
    "\n",
    "\n",
    "# target tensor : 1 words, 1x16;\n",
    "# decoder output : 1 words,\n",
    "\n",
    "print(\"target_length is randomized\", target_length)\n",
    "\n",
    "# Teacher forcing: Feed the target as the next input\n",
    "if use_teacher_forcing:\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        decoder_output_2 = decoder_output\n",
    "        aggregate_outputs[di] = decoder_output\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "# Without teacher forcing: use its own predictions as the next input\n",
    "else:\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "encoder_c_optimizer.step()\n",
    "encoder_a_optimizer.step()\n",
    "decoder_optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(target_length)\n",
    "print(target_length)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion: nn.NLLLoss = nn.NLLLoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aggregate_outputs.size()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aggregate_outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtarget_tensor\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'target_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "target_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_test = criterion(aggregate_outputs, trg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "OUTPUT_DIM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for text in training_pairs:\n",
    "    target_sentece_1 = text[2]\n",
    "    print(len(target_sentece_1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outputs = torch.zeros(\n",
    "    target_length,\n",
    "    OUTPUT_DIM ,\n",
    "    device=device)\n",
    "print(decoder_output.size())\n",
    "decoder_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(target_tensor.size())\n",
    "target_tensor\n",
    "\n",
    "# the input for criterion is target_tensor[i] which means = it take the vector of the words target;\n",
    "# compare with decoder  of len of the vocab size;"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "di = 2\n",
    "target_tensor[di]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ngram_maker(list_sentences, min_length, max_length):\n",
    "    lengths = range(min_length, max_length + 1)\n",
    "    res_ngram = {length : [] for length in lengths}\n",
    "\n",
    "    for leng in lengths:\n",
    "        for sentence in list_sentences:\n",
    "            n_grams = ngrams(nltk.word_tokenize(sentence), leng)\n",
    "            for gram in n_grams:\n",
    "                temp_word = ' '.join(gram)\n",
    "                res_ngram[leng].append(temp_word)\n",
    "\n",
    "    return res_ngram"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_attribute_markers_test(s, style_src):\n",
    "    sentence = [s]\n",
    "\n",
    "    ngrams = get_counts(sentence, d_nonclick_ngrams_counts)\n",
    "    if len(ngrams) > 0:\n",
    "        print(\"before\", ngrams)\n",
    "        ngrams = ngrams[:,1]\n",
    "        print(\"after\", ngrams)\n",
    "\n",
    "    pos_counts = get_counts(sentence, d_nonclick_ngrams_counts)\n",
    "\n",
    "    if len(pos_counts) > 0:\n",
    "        pos_counts = pos_counts[:,0]\n",
    "        # print(pos_counts)\n",
    "\n",
    "    neg_counts = get_counts(sentence, d_clickbait_ngrams_counts)\n",
    "    if len(neg_counts) > 0:\n",
    "        neg_counts = neg_counts[:,0]\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "ttt = get_attribute_markers_test(\"bjd_nonclick_ngrams_counts habibie meningggal\", 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "param_span_t = 3\n",
    "ngram_stc_only = ngram_from_sentence_v3(\"bj habibie meninggal\", 1, 2)\n",
    "res = count_ngram_sentence_onlyV3(non_clickbait_ngram_count_v2, ngram_stc_only,1, 2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a_elements = importances_t[0][1]\n",
    "a_original = a\n",
    "az = array_to_string(a_elements).split()\n",
    "a_test =  np.isin(az, array_to_string(a_original).split()).any()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "min_length = 1\n",
    "max_length = 2\n",
    "ref_bait = \"Clickbait\"\n",
    "param_threshold= 1\n",
    "sentence = \"viral ! ini ! itu bj habibie \"\n",
    "\n",
    "smoothing_parameter = 1\n",
    "\n",
    "sentence_ngram_ref = ngram_from_sentence_v3(ori_sentence=sentence, min_length=min_length, max_length=max_length)\n",
    "\n",
    "# count the clickbait sentence, clickbait ngram count as based reference\n",
    "counts_sentence_clickbait = count_ngram_sentence_onlyV3(clickbait_ngram_count_v2,sentence_ngram_ref,min_length,max_length)\n",
    "\n",
    "# count the non-clickbait sentence, non-clickbait ngram count as based reference\n",
    "counts_sentence_nonclickbait = count_ngram_sentence_onlyV3(non_clickbait_ngram_count_v2,sentence_ngram_ref,min_length,max_length)\n",
    "\n",
    "importances = 0\n",
    "\n",
    "csn_int = counts_sentence_nonclickbait[:,1]\n",
    "csn_ngram = counts_sentence_nonclickbait[:,0]\n",
    "\n",
    "csc_int = counts_sentence_clickbait[:,1]\n",
    "csc_ngram = counts_sentence_clickbait[:,0]\n",
    "\n",
    "if ref_bait == \"Clickbait\":\n",
    "    top = (csc_int + smoothing_parameter)\n",
    "    bottom = (csn_int + smoothing_parameter)\n",
    "    importances = (top/bottom)\n",
    "elif ref_bait == \"NotClickbait\":\n",
    "    top = (csn_int+ smoothing_parameter)\n",
    "    bottom = (csc_int + smoothing_parameter)\n",
    "    importances = (top/bottom)\n",
    "\n",
    "# print(sentence_ngram_ref)\n",
    "\n",
    "de = counts_sentence_clickbait[:, 0]\n",
    "importances_t = np.vstack((importances, de)).T\n",
    "\n",
    "def array_to_string(a):\n",
    "    return ' '.join(flatten(a))\n",
    "\n",
    "def is_in_string_array_v2(elements, original): #deprecated, does not take into account sequence order\n",
    "    return np.isin(array_to_string(elements).split(), array_to_string(original).split()).any()\n",
    "\n",
    "a = []\n",
    "for importance in importances_t:\n",
    "    if importance[0] > param_threshold and not is_in_string_array_v2(importance[1], a):\n",
    "        a.append(' '.join(importance[1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def is_in_string_array(elements, original): #deprecated, does not take into account sequence order\n",
    "\n",
    "    print(\"element: \", elements)\n",
    "    print(\"original reference list: \", original)\n",
    "\n",
    "    atse = array_to_string(elements)\n",
    "    atses = atse.split()\n",
    "\n",
    "    print('atse :  ', atse)\n",
    "    print('atses : ', atses)\n",
    "\n",
    "    atsos = array_to_string(original).split()\n",
    "\n",
    "    return np.isin(atses, atsos ).any()\n",
    "# no usage in the func\n",
    "\n",
    "a = []\n",
    "for importance in importances_t:\n",
    "    print(\"new element =====\")\n",
    "    if importance[0] > param_threshold and not is_in_string_array_v2(importance[1], a):\n",
    "\n",
    "        print(\" insert to a, not in the string array : \", importance[1])\n",
    "        a.append(' '.join(importance[1]))\n",
    "    else:\n",
    "        print(\"nah no input to a\")\n",
    "        print(\"==============\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get ngram from the word in the sentence reference only\n",
    "def ngram_from_sentence_v3(ori_sentence, min_length, max_length):\n",
    "    sent_in_arr = [ori_sentence]\n",
    "    lengths = range(min_length, max_length + 1)\n",
    "    res_ngram_sentence = []\n",
    "\n",
    "    for leng in lengths:\n",
    "        for sentence in sent_in_arr:\n",
    "            n_grams = ngrams(nltk.word_tokenize(sentence), leng)\n",
    "            for grams in n_grams:\n",
    "                res = ' '.join(grams)\n",
    "                if res not in res_ngram_sentence:\n",
    "                    res_ngram_sentence.append(res)\n",
    "\n",
    "            sentence_split = sentence.split()\n",
    "            n_grams_split = nltk.ngrams(sentence_split, leng)\n",
    "\n",
    "            for grams in n_grams_split:\n",
    "                res = ' '.join(grams)\n",
    "                if res not in res_ngram_sentence:\n",
    "                    res_ngram_sentence.append(res)\n",
    "\n",
    "    return res_ngram_sentence\n",
    "\n",
    "# get  counter based on the sentence and compare to the ngram dictionary\n",
    "def count_ngram_sentence_only(ngram_dictionary, ngram_from_sentence, min_length, max_length):\n",
    "    lengths = range(min_length, max_length + 1)\n",
    "\n",
    "    ngram_compare_dict = {}\n",
    "    ngram_compare_arr = []\n",
    "\n",
    "    for key in lengths:\n",
    "        for split in ngram_from_sentence:\n",
    "            # print(split)\n",
    "            for ngram_list in ngram_dictionary[key].items():\n",
    "                word_freq = ngram_list[0]\n",
    "                if split == word_freq:\n",
    "                    if split in ngram_compare_dict:\n",
    "                        ngram_compare_dict[split] = max(ngram_compare_dict[split], ngram_dictionary[key][split])\n",
    "                    ngram_compare_dict[split] = ngram_dictionary[key][split]\n",
    "                elif split not in ngram_compare_dict:\n",
    "                    ngram_compare_dict[split] = 0\n",
    "\n",
    "    for key, val in ngram_compare_dict.items():\n",
    "        ngram_compare_arr.append([key, val])\n",
    "\n",
    "    return np.array(ngram_compare_arr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "param_smooth = 1\n",
    "param_threshold = 5\n",
    "param_span = 4\n",
    "\n",
    "param_backoff_limit = 3\n",
    "\n",
    "#ngram has punctuation\n",
    "def has_punctuation(ngram): #damn I'm very proud of making this from scratch lol, looks elegant in one line\n",
    "    return True in [x in string.punctuation for x in ngram]\n",
    "\n",
    "# generate ngram from all of the sentence\n",
    "def generate_ngrams(lines,  min_length: int =1, max_length=param_span) -> dict :\n",
    "    #     lines = placeholder + lines\n",
    "    lengths: range  = range(min_length, max_length + 1)\n",
    "    ngrams: dict  = {length: [] for length in lengths}\n",
    "    queue: collections.deque = collections.deque(maxlen=max_length)\n",
    "\n",
    "    def add_queue():\n",
    "        current = tuple(queue)\n",
    "        for length in lengths:\n",
    "            if len(current) >= length and not has_punctuation(current[:length]):\n",
    "                ngrams[length].append(current[:length])\n",
    "\n",
    "    short_by = 0\n",
    "    for line in lines:\n",
    "        short_by = max(0, max_length - len(lines))\n",
    "        for word in line.split():\n",
    "            queue.append(word)\n",
    "            if len(queue) >= max_length-short_by:\n",
    "                add_queue()\n",
    "\n",
    "    while len(queue) > min_length:\n",
    "        queue.popleft()\n",
    "        add_queue()\n",
    "    return ngrams\n",
    "\n",
    "#modified from & fixed their error of ngram with # of words < 4: https://gist.github.com/benhoyt/dfafeab26d7c02a52ed17b6229f0cb52\n",
    "def count_ngrams(lines, min_length: int =1, max_length=param_span ) -> dict:\n",
    "    \"\"\"Iterate through given lines iterator (file object or list of\n",
    "    lines) and return n-gram frequencies. The return value is a dict\n",
    "    mapping the length of the n-gram to a collections.Counter\n",
    "    object of n-gram tuple and number of times that n-gram occurred.\n",
    "    Returned dict includes n-grams of length min_length to max_length.\n",
    "    \"\"\"\n",
    "    lengths = range(min_length, max_length + 1)\n",
    "    ngrams = {length: collections.Counter() for length in lengths}\n",
    "    queue = collections.deque(maxlen=max_length)\n",
    "\n",
    "    # Helper function to add n-grams at start of current queue to dict\n",
    "    def add_queue():\n",
    "        current = tuple(queue)\n",
    "        for length in lengths:\n",
    "            if len(current) >= length and not has_punctuation(current[:length]):\n",
    "                ngrams[length][current[:length]] += 1\n",
    "\n",
    "    # Loop through all lines and words and add n-grams to dict\n",
    "    short_by = 0\n",
    "    for line in lines:\n",
    "        short_by = max(0, max_length - len(lines))\n",
    "        for word in line.split():\n",
    "            queue.append(word)\n",
    "            if len(queue) >= max_length - short_by:\n",
    "                add_queue()\n",
    "\n",
    "    # Make sure we get the n-grams at the tail end of the queue\n",
    "    while len(queue) > min_length:\n",
    "        queue.popleft()\n",
    "        add_queue()\n",
    "\n",
    "    return ngrams\n",
    "\n",
    "#Example of usage: count ngram of the list of sentence\n",
    "d_nonclick_ngrams_counts = count_ngrams(nonclickbait_l_c)\n",
    "d_clickbait_ngrams_counts = count_ngrams(clickbait_l_c)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PRE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# testing example\n",
    "stc_rand: [] = random.choice(pairs)\n",
    "print(\"content : {} \\nattribute: {} \\ntgt: {} \".format(stc_rand[0], stc_rand[1], stc_rand[2]))\n",
    "\n",
    "training_pairs = [tensorsFromPair(stc_rand) for i in range(10)]\n",
    "# print(len(training_pairs))\n",
    "\n",
    "iter = 8\n",
    "training_pair = training_pairs[iter - 1]\n",
    "input_tensor_c = training_pair[0]\n",
    "# input_tensor_a = training_pair[1]\n",
    "# target_tensor = training_pair[2]\n",
    "\n",
    "\n",
    "# print(len(input_tensor_c))\n",
    "# print(input_tensor_c)\n",
    "\n",
    "# input_c_length = input_tensor_c.size(0)\n",
    "\n",
    "# print(input_c_length)\n",
    "\n",
    "# prev size\n",
    "word_vec_size = 128\n",
    "hidden_size = 512\n",
    "\n",
    "# word_size map of the input and the output of the words\n",
    "INPUT_DIM = input_lang.n_words\n",
    "OUTPUT_DIM = output_lang.n_words\n",
    "\n",
    "# dimension_size of embedding layers\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "\n",
    "# hidden_size of the dimension that connect between input, hidden,output\n",
    "HID_DIM = 512\n",
    "NN_LAYERS = 2\n",
    "\n",
    "\n",
    "# implementation in encoder both A and C\n",
    "encoder_c = EncoderRNN(INPUT_DIM, ENC_EMB_DIM, HID_DIM).to(device)\n",
    "encoder_a = EncoderRNN(INPUT_DIM, DEC_EMB_DIM, HID_DIM).to(device)\n",
    "\n",
    "# decoder\n",
    "decoder = DecoderRNN(HID_DIM + HID_DIM, word_vec_size, OUTPUT_DIM).to(device)\n",
    "\n",
    "max_length = 50\n",
    "\n",
    "encoder_c_outputs = torch.zeros(max_length, encoder_c.hidden_size, device=device)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "24"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lambda x: x*2)(12)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<filter at 0x7f927e9e70a0>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_1 = [1,2,3,4,5,6,7,8,9]\n",
    "filter(lambda x: x%2==0, list_1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[2, 4, 6, 8]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x%2==0, list_1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 8, 27, 64, 125, 216, 343, 512, 729]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_1 = [1,2,3,4,5,6,7,8,9]\n",
    "cubed = map(lambda x: pow(x,3), list_1)\n",
    "list(cubed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "   Name    Status  Birthyear\n0  Luke    Father       1976\n1  Gina    Mother       1984\n2   Sam       Son       2013\n3  Emma  Daughter       2016",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Status</th>\n      <th>Birthyear</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Luke</td>\n      <td>Father</td>\n      <td>1976</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Gina</td>\n      <td>Mother</td>\n      <td>1984</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Sam</td>\n      <td>Son</td>\n      <td>2013</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Emma</td>\n      <td>Daughter</td>\n      <td>2016</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Name': ['Luke','Gina','Sam','Emma'],\n",
    "    'Status': ['Father', 'Mother', 'Son', 'Daughter'],\n",
    "    'Birthyear': [1976, 1984, 2013, 2016],\n",
    "})\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "df['age'] = df['Birthyear'].apply(lambda x: 2021-x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method NDFrame.head of 0    45\n1    37\n2     8\n3     5\nName: age, dtype: int64>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.age.head"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "[45, 37]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(filter(lambda x: x>18, df['age']))\n",
    "list(filter(lambda x: x> 18, df.age))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df['double_age'] = df['age'].map(lambda x: x*2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "df['Gender'] = df['Status'].map(lambda x: 'Male' if x=='Father' or x=='son' else 'Female')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "   Name    Status  Birthyear  age  double_age  Gender\n0  Luke    Father       1976   45          90    Male\n1  Gina    Mother       1984   37          74  Female\n2   Sam       Son       2013    8          16  Female\n3  Emma  Daughter       2016    5          10  Female",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Status</th>\n      <th>Birthyear</th>\n      <th>age</th>\n      <th>double_age</th>\n      <th>Gender</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Luke</td>\n      <td>Father</td>\n      <td>1976</td>\n      <td>45</td>\n      <td>90</td>\n      <td>Male</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Gina</td>\n      <td>Mother</td>\n      <td>1984</td>\n      <td>37</td>\n      <td>74</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Sam</td>\n      <td>Son</td>\n      <td>2013</td>\n      <td>8</td>\n      <td>16</td>\n      <td>Female</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Emma</td>\n      <td>Daughter</td>\n      <td>2016</td>\n      <td>5</td>\n      <td>10</td>\n      <td>Female</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bruce', 'Wayne', 'Is', 'Batman']\n",
      "['bruce', 'wayne', 'is', 'batman']\n",
      "bruce wayne is batman\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def putSpace(input):\n",
    "\n",
    "    # regex [A-Z][a-z]* means any string starting\n",
    "    # with capital character followed by many\n",
    "    # lowercase letters\n",
    "    words = re.findall('[A-Z][a-z]*', input)\n",
    "\n",
    "    print(words)\n",
    "\n",
    "    # Change first letter of each word into lower\n",
    "    # case\n",
    "    for i in range(0,len(words)):\n",
    "        words[i]=words[i][0].lower()+words[i][1:]\n",
    "\n",
    "    print(words)\n",
    "    print(' '.join(words))\n",
    "\n",
    "# Driver program\n",
    "if __name__ == \"__main__\":\n",
    "    input = 'BruceWayneIsBatman'\n",
    "    putSpace(input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "text_try = \"digrebeg di hotel, aktor ftv ridho illahi berduaan dengan selebgram chagii wow! amelia?\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['Status'] = df['Status'].replace({'.':' . ',', ':' , ', '!': \" ! \", \"?\": \" ? \"},regex=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text_try = text_try.replace(\".\", \" . \" )\n",
    "\n",
    "res_text = text_try.replace(\",\", \" , \" )\n",
    "\n",
    "# res_text = text_try.replace({'.':' . ',', ':' , ', '!': \" ! \", \"?\": \" ? \"}, )\n",
    "\n",
    "print(res_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rule = \"([\\D])([^\\w^\\s])([\\s$\\D])?\"\n",
    "\n",
    "text = \"sering quality time bersama keluarga? ternyata ini 3,000 man,3 seperti manfaatnya!\"\n",
    "\n",
    "clean = re.compile(rule)\n",
    "output = re.sub(clean, '\\1 \\2 \\3' , text)\n",
    "\n",
    "print(output)\n",
    "# res = text.apply(lambda y: \" \".join((re.sub(r'([!/?/./,/%/\\'/\\\"/-/:])', lambda x: ' ' + x.group()+' ' , y)).split()) , 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chapter 16 – Natural Language Processing with RNNs and Attention"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 7)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge tensorflow -y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "assert tf.__version__ >= \"2.8.0\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# !conda remove tensorflow -y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (2.9.2)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (0.26.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: setuptools in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (63.4.1)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (1.14.1)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (1.48.1)\r\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (1.12)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (1.2.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (3.7.0)\r\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (3.19.4)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (2.9.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (1.16.0)\r\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (2.9.1)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (1.1.2)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (14.0.6)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (1.23.2)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (1.1.0)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (0.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (4.3.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (2.9.0)\r\n",
      "Requirement already satisfied: packaging in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow) (21.3)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.11.0)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.28.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.2.2)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from packaging->tensorflow) (3.0.9)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.12.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.6.15)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.11)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.8.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# !pip3 install --upgrade tensorflow==2.8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "assert tf.__version__ >= \"2.8.0\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# !python -m pip show tensorflow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"nlp\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. Neural nets can be very slow without a GPU.\n"
     ]
    }
   ],
   "source": [
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
    "              \"accelerator.\")\n",
    "    if \"kaggle_secrets\" in sys.modules:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "#\n",
    "# assert tf.__version__ >= \"2.8.0\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "# extra code – shows a short text sample\n",
    "print(shakespeare_text[:80])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code – shows all 39 distinct characters (after converting to lower case)\n",
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
    "#                                                    standardize=\"lower\")\n",
    "# text_vec_layer.adapt([shakespeare_text])\n",
    "# encoded = text_vec_layer([shakespeare_text])[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# !pip3 uninstall tf-nightly -y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "assert tf.__version__ >= \"2.8.0\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 20:05:42.108456: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.layers.preprocessing.text_vectorization.TextVectorization at 0x7f838ddb9130>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.TextVectorization()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
    "                                                   standardize=\"lower\")\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "encoded -= 2  # drop tokens 0 (pad) and 1 (unknown), which we will not use\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\n",
    "dataset_size = len(encoded)  # total number of chars = 1,115,394"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19,  5,  8, ..., 20, 26, 10])>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "<keras.layers.preprocessing.text_vectorization.TextVectorization at 0x7f838df80370>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "39"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "1115394"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "[(<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 4,  5,  2, 23]])>,\n  <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[ 5,  2, 23,  3]])>)]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extra code – a simple example using to_dataset()\n",
    "# There's just one sample in this dataset: the input represents \"to b\" and the\n",
    "# output represents \"o be\"\n",
    "list(to_dataset(text_vec_layer([\"To be\"])[0], length=4))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "length = 100\n",
    "tf.random.set_seed(42)\n",
    "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True,\n",
    "                       seed=42)\n",
    "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
    "test_set = to_dataset(encoded[1_060_000:], length=length)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 20:06:58.077319: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 72915 of 100000\n",
      "2022-09-09 20:07:02.071507: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:415] Shuffle buffer filled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [31]\u001B[0m, in \u001B[0;36m<cell line: 15>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse_categorical_crossentropy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      7\u001B[0m               optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnadam\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      8\u001B[0m               metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     10\u001B[0m model_ckpt \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mcallbacks\u001B[38;5;241m.\u001B[39mModelCheckpoint(\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmy_shakespeare_model\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     12\u001B[0m     monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval_accuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     13\u001B[0m     save_best_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 15\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_set\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalid_set\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mmodel_ckpt\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[1;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/keras/engine/training.py:1409\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1402\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1403\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   1404\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   1405\u001B[0m     step_num\u001B[38;5;241m=\u001B[39mstep,\n\u001B[1;32m   1406\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m   1407\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m   1408\u001B[0m   callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1409\u001B[0m   tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1410\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1411\u001B[0m     context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:980\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    976\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# Fall through to cond-based initialization.\u001B[39;00m\n\u001B[1;32m    977\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    978\u001B[0m     \u001B[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001B[39;00m\n\u001B[1;32m    979\u001B[0m     \u001B[38;5;66;03m# stateless function.\u001B[39;00m\n\u001B[0;32m--> 980\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stateless_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    981\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    982\u001B[0m   _, _, filtered_flat_args \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    983\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn\u001B[38;5;241m.\u001B[39m_function_spec\u001B[38;5;241m.\u001B[39mcanonicalize_function_inputs(  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    984\u001B[0m           \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds))\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2453\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2450\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m   2451\u001B[0m   (graph_function,\n\u001B[1;32m   2452\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[0;32m-> 2453\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2454\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1860\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1856\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1857\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1858\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1859\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1860\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1861\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1862\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1863\u001B[0m     args,\n\u001B[1;32m   1864\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1865\u001B[0m     executing_eagerly)\n\u001B[1;32m   1866\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow/python/eager/function.py:497\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    495\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    496\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 497\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    503\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    504\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m    505\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[1;32m    506\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    509\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[1;32m    510\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_shakespeare_model\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(train_set,\n",
    "                    validation_data=valid_set,\n",
    "                    epochs=1,\n",
    "                    callbacks=[model_ckpt])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# An Encoder–Decoder Network for Neural Machine Translation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\",\n",
    "                               extract=True)\n",
    "text = (Path(path).with_name(\"spa-eng\") / \"spa.txt\").read_text()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
    "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "np.random.seed(42)  # extra code – ensures reproducibility on CPU\n",
    "np.random.shuffle(pairs)\n",
    "sentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "[['How boring!', 'Qué aburrimiento!'],\n ['I love sports.', 'Adoro el deporte.'],\n ['Would you like to swap jobs?',\n  'Te gustaría que intercambiemos los trabajos?'],\n ['My mother did nothing but weep.', 'Mi madre no hizo nada sino llorar.'],\n ['Croatia is in the southeastern part of Europe.',\n  'Croacia está en el sudeste de Europa.'],\n ['I have never eaten a mango before.', 'Nunca he comido un mango.'],\n ['Tell the taxi driver to drive faster.',\n  'Decile al taxista que maneje más rápido.'],\n ['Tom and I work together.', 'Tom y yo trabajamos juntos.'],\n ['I would prefer an honorable death.', 'Preferiría una muerte honorable.'],\n ['Tom married a much younger woman.',\n  'Tom se ha casado con una mujer mucho más joven.'],\n [\"It couldn't happen to me.\", 'A mí no podría pasarme.'],\n [\"Tom isn't going to marry you.\", 'Tom no se va a casar contigo.'],\n [\"Can you believe it? He's even lazier than me.\",\n  'Puedes creerlo? Él es aún más flojo que yo.'],\n ['She has hundreds of books.', 'Ella tiene cientos de libros.'],\n ['They found him guilty.', 'Lo encontraron culpable.'],\n ['Tom calls Mary up every night.', 'Tom llama todas las noches a Mary.'],\n [\"Tom says he won't come.\", 'Tom dice que no va a venir.'],\n [\"I don't mind if the weather is hot.\", 'No me importa que haga calor.'],\n ['It is difficult to speak Chinese well.', 'Es difícil hablar bien chino.'],\n [\"Tom claimed that he didn't know Mary.\", 'Tom decía no conocer a Mary.'],\n ['Just come up here.', 'Venga, ven aquí.'],\n ['Tom has a strange way of talking.', 'Tom tiene una forma rara de hablar.'],\n ['Tom has many friends living in Boston.',\n  'Tom tiene muchos amigos viviendo en Boston.'],\n ['I made my decision.', 'Tomé mi decisión.'],\n ['Come again tomorrow.', 'Vuelvan mañana.'],\n [\"I think I'm still drunk from last night.\",\n  'Creo que todavía estoy borracho de anoche.'],\n ['My father and my brother work in this factory.',\n  'Mi padre y mi hermano trabajan en esta fábrica.'],\n ['We need your signature.', 'Necesitamos su firma.'],\n [\"Learn from others' mistakes.\", 'Aprende de los errores de los demás.'],\n ['The dog walked backward.', 'El perro caminaba para atrás.'],\n [\"I don't want to drink anything.\", 'No quiero beber nada.'],\n ['Everybody likes him.', 'Todos lo quieren.'],\n ['My father disapproved of my going to the concert.',\n  'Mi padre se opuso a que fuera al concierto.'],\n ['She is a singer.', 'Ella es cantante.'],\n ['Tom has something in his right hand.',\n  'Tom tiene algo en su mano derecha.'],\n ['She is a leader in her field.', 'Es una autoridad en su campo.'],\n [\"They're sunbathing around the pool.\",\n  'Ellas están tomando sol alrededor de la piscina.'],\n [\"I think Tom didn't understand the joke.\",\n  'Creo que Tomás no entendió la broma.'],\n [\"I don't like doctors.\", 'No me gustan los médicos.'],\n [\"Don't laugh at him.\", 'No te rías de él.'],\n ['The enemy attacked from behind.', 'El enemigo atacó por detrás.'],\n ['Is Tom famous?', 'Tom es famoso?'],\n ['I asked him if I could read his book.',\n  'Le pregunté si podía leer su libro.'],\n ['I have a slight pain in my side.', 'Tengo un ligero dolor en mi costado.'],\n [\"You don't speak English, do you?\", 'Tú no hablas inglés, verdad?'],\n ['They are more or less the same size.', 'Son más o menos del mismo tamaño.'],\n ['Neither of these is mine.', 'Ninguno de éstos es mío.'],\n ['Tom is the person to ask.', 'Tom es la persona a preguntar.'],\n [\"I don't like this shirt. Show me another one.\",\n  'Esta camisa no me gusta. Enséñame otra.'],\n ['Tom wanted to know who Mary had been dating.',\n  'Tom quería saber con quién había estado saliendo Mary.'],\n [\"I didn't think this was your seat.\", 'No pensé que fuera tu asiento.'],\n ['The glass is dirty.', 'El vaso está sucio.'],\n [\"You didn't want to tell Tom about that, did you?\",\n  'No querías contárselo a Tom, verdad?'],\n [\"You've misunderstood.\", 'Lo habéis entendido mal.'],\n ['Tom cheated.', 'Tomás hizo trampa.'],\n ['The doctor took my pulse.', 'El doctor me tomó el pulso.'],\n ['Tips are not accepted.', 'No se aceptan propinas.'],\n [\"It's your turn to read.\", 'Es tu turno de leer.'],\n [\"I wasn't careful.\", 'No fui preocupado.'],\n ['Enjoy your meal.', 'Que aproveche.'],\n [\"I don't mind sharing the room with him.\",\n  'No me molesta compartir la habitación con él.'],\n [\"You know me well enough to know I wouldn't do that.\",\n  'Me conoces lo suficientemente bien para saber que yo no lo haría.'],\n ['You should try the exam again.',\n  'Deberías volver a presentarte al examen.'],\n ['He shot at the bird, but missed it.',\n  'Él le disparó al pájaro, sin embargo falló.'],\n [\"I don't want trouble.\", 'No quiero problemas.'],\n ['Tom has no memory of what happened.',\n  'Tom no tiene recuerdos de lo que ocurrió.'],\n [\"I don't know why I bother coming here.\",\n  'No sé por qué me molesto en venir aquí.'],\n ['Is this a bad time to chat?', 'Es mal momento para charlar?'],\n ['There were no radios in those days.', 'No había radios aún entonces.'],\n ['Does Tom have a girlfriend?', 'Tom tiene novia?'],\n [\"The defendant's innocence could not be verified.\",\n  'No se pudo comprobar la inocencia del acusado.'],\n ['Who told you to bring me here?', 'Quién te ha dicho que me traigas aquí?'],\n ['What was there?', 'Qué ha sido eso?'],\n [\"You told me it wouldn't rain today, so I didn't bring an umbrella.\",\n  'Me dijiste que no iba a llover hoy, así que no traje un paraguas.'],\n [\"I don't like to be disturbed.\", 'No me gusta que me molesten.'],\n [\"Tom doesn't trust us.\", 'Tom no confía en nosotros.'],\n ['Please forgive me.', 'Por favor, perdóname.'],\n ['They will survey the desert island.',\n  'Ellos inspeccionarán la isla desierta.'],\n ['Can you speak a little slower?', 'Puede hablar un poco más despacio?'],\n [\"Don't underestimate the problem.\", 'No subestimes el problema.'],\n ['Tom took my car.', 'Tom cogió mi coche.'],\n [\"We didn't do anything wrong.\", 'No hicimos nada malo.'],\n ['It makes no difference to me whether he comes or not.',\n  'Me da igual que venga o no.'],\n ['I love weddings.', 'Me encantan las bodas.'],\n ['I got jealous.', 'Yo me puse celoso.'],\n [\"I think it's time for me to discuss the problem with her.\",\n  'Creo que es hora de que discuta el problema con ella.'],\n ['Tom comes here every day.', 'Tom viene aquí todos los días.'],\n ['Just ignore her.', 'Solo ignórala.'],\n [\"There are few men who don't know that.\",\n  'Hay pocos hombres que no lo saben.'],\n ['Let me pay my share.', 'Permitime pagar mi parte.'],\n [\"I've come back for you.\", 'He vuelto por ti.'],\n ['Those books are mine.', 'Son mis libros.'],\n ['The restaurant is next door to the theater.',\n  'El restaurante está al lado del teatro.'],\n ['I know what that means.', 'Yo sé lo que eso significa.'],\n ['She is good at speaking English.', 'Ella destaca en conversación inglesa.'],\n [\"He's always joking.\", 'Está siempre bromeando.'],\n ['What did you do with that camera?', 'Qué hiciste con esa cámara?'],\n ['Just make this stop.', 'Haz que pare.'],\n ['I saw many familiar faces.', 'Vi muchas caras familiares.'],\n ['If you are going to go to America, you should brush up your English.',\n  'Si vas a ir a los Estados Unidos, deberías refrescar tu inglés.'],\n ['Tom lives in the room above us.',\n  'Tom vive en la habitación encima de la nuestra.'],\n ['Are you suggesting we run and hide?',\n  'Estás sugiriendo que corramos y nos escondamos?'],\n ['She avoided answering my questions.',\n  'Ella evitó responder mis preguntas.'],\n ['There is a great demand for gasoline.', 'Hay mucha demanda de gasolina.'],\n ['My sister usually goes to the park every weekend.',\n  'Mi hermana suele ir al parque los fines de semana.'],\n ['Always keep a handkerchief in your pocket.',\n  'Mantén siempre un pañuelo en tu bolsillo.'],\n [\"Please don't look over here again.\",\n  'Por favor, no vuelvas a mirar hacia aquí.'],\n [\"I'm intrigued.\", 'Estoy intrigada.'],\n ['I want to leave early.', 'Quiero irme temprano.'],\n ['The flowers in the vase were wilted.',\n  'Las flores del jarrón se han marchitado.'],\n [\"I don't like talking about football.\", 'No me gusta hablar de fútbol.'],\n ['I could kill you.', 'Yo podría matarlos.'],\n ['This is smaller than that.', 'Esto es más pequeño que eso.'],\n ['Mary has just come home.', 'Mary acaba de llegar a casa.'],\n ['I was concerned.', 'Estaba preocupado.'],\n ['I found the room empty.', 'Encontré la habitación vacía.'],\n ['Tom bought a piece of land not far from where Mary lives.',\n  'Tom compró un terreno cerca de donde vive Mary.'],\n ['Is this your dog?', 'Es este su perro?'],\n [\"Why don't you just shut up?\", 'Por qué no te callás?'],\n ['Tom felt sick.', 'Tom se sentía enfermo.'],\n ['Are they satisfied?', 'Están satisfechos?'],\n ['Do you know anyone who hums while they work?',\n  'Conoces a alguien que tataree mientras trabaja?'],\n ['You lucky devil!', 'Qué suertudo!'],\n [\"He got the twelve o'clock train.\", 'Él tomó el tren de las doce.'],\n [\"Tom isn't a crook.\", 'Tom no es un pillo.'],\n ['Nobody was tortured.', 'No se torturó a nadie.'],\n ['Tom wanted to help Mary.', 'Tom quería ayudar a Mary.'],\n ['It looks like Tom has an alibi for the night Mary was murdered.',\n  'Parece que Tom tiene una coartada para la noche en que Mary fue asesinada.'],\n ['I bought a hat at the store.', 'Compré un sombrero en la tienda.'],\n ['My sister is always weighing herself.',\n  'Mi hermana se está controlando el peso continuamente.'],\n ['He seems to be asleep.', 'Parece estar dormido.'],\n ['What time did you get here this morning?',\n  'A qué hora has llegado esta mañana?'],\n [\"I'm pretty hungry since I haven't eaten since early this morning.\",\n  'Tengo bastante hambre porque no he comido desde esta mañana temprano.'],\n ['Tom is a former paratrooper.', 'Tom es un antiguo paracaidista.'],\n ['We understand this.', 'Comprendemos esto.'],\n ['He was pleased with the toy.', 'Él estaba contento con el juguete.'],\n ['Tom has a reservation at this hotel.',\n  'Tom tiene una reserva en este hotel.'],\n [\"Just don't tell anyone else.\", 'Pero no le digas a nadie más.'],\n ['Tom followed us.', 'Tom nos siguió.'],\n ['You are such a liar!', 'Eres tan mentiroso!'],\n ['Are you single?', 'Estás soltero?'],\n [\"Don't worry about it. Everything's going to be fine.\",\n  'No preocupes por eso. Todo saldrá bien.'],\n ['I called him, but a girl answered the phone.',\n  'Lo llamé, pero una niña contestó el teléfono.'],\n ['She was asked to convince him to get his son to paint the house.',\n  'Le pidieron que lo convenciera para que su hijo pintara la casa.'],\n ['I like to relax with a good novel.',\n  'Me gusta relajarme con una buena novela.'],\n ['You should get your car fixed.',\n  'Ustedes deberían hacer reparar su coche.'],\n ['He is still here.', 'Todavía está aquí.'],\n [\"You're very astute.\", 'Sos muy astuto.'],\n ['She hurt her elbow when she fell down.',\n  'Se lastimó su codo cuando se cayó.'],\n ['Hold the ball with both hands.', 'Sostén la pelota con ambas manos.'],\n ['Do you like Renaissance art?', 'Te gusta el arte renacentista?'],\n ['Stop that car.', 'Para ese coche.'],\n [\"From this point, we'll go on foot.\", 'De aquí en adelante iremos a pie.'],\n ['Tom had a very good time.', 'Tom se divirtió mucho.'],\n [\"I told you I don't know if Tom will tell the truth.\",\n  'Ya te he dicho que no sé si Tom te dirá la verdad.'],\n ['She fell asleep with her sweater on.',\n  'Ella se quedó dormida con el jersey puesto.'],\n ['Tom stood up and headed for the bathroom.',\n  'Tom se paró y se dirigió al baño.'],\n ['Do you like them?', 'Te gustan?'],\n ['I noticed that she was wearing new glasses.',\n  'Me di cuenta de que ella llevaba gafas nuevas.'],\n ['I never have had occasion to use it.',\n  'No he tenido nunca la oportunidad de ocuparlo.'],\n [\"I've lost my ticket.\", 'He perdido mi boleto.'],\n ['Who else can answer my question?',\n  'Quién más puede contestar a mi pregunta?'],\n [\"Don't bother me with such trifles.\",\n  'No me molestes por semejantes nimiedades.'],\n ['I emailed Tom the pictures I took yesterday.',\n  'Le envié por correo electrónico a Tom las fotografías que tomé ayer.'],\n ['He made no response.', 'Él no dio una respuesta.'],\n ['I hate the desert.', 'Odio el desierto.'],\n ['You have until midnight.', 'Tienen hasta la medianoche.'],\n ['One of my friends says he wants me to meet you.',\n  'Uno de mis amigos dice que quiere conocerte.'],\n ['Tom told his son the story about a monster that ate children.',\n  'Tom le contó a su hijo la historia de un monstruo que comía niños.'],\n [\"Don't eat too much.\", 'No comas demasiado.'],\n ['She has been watching television for three hours.',\n  'Ella ha estado viendo televisión por tres horas.'],\n ['Can I try on this jacket?', 'Me puedo probar esta chaqueta?'],\n [\"I don't care.\", 'No me importa.'],\n ['Tom changed his mind at the last minute.',\n  'Tom cambió de idea en el último minuto.'],\n [\"Don't leave the bedroom window open.\",\n  'No dejes la ventana del cuarto abierta.'],\n ['Try some.', 'Prueben un poco.'],\n ['Is anybody hurt?', 'Se lastimó alguien?'],\n ['His name escapes me.', 'Su nombre no lo recuerdo.'],\n ['She was busy with her knitting.', 'Ella estaba muy ocupada tejiendo.'],\n [\"I wish the subway wasn't so crowded every morning.\",\n  'Ojalá el metro no estuviera tan lleno todas las mañanas.'],\n [\"Maybe there's something I can do.\",\n  'Tal vez haya algo que yo pueda hacer.'],\n [\"I can't believe you married Tom.\",\n  'No me puedo creer que te casaras con Tom.'],\n ['I see a lion.', 'Veo un león.'],\n [\"This woman definitely knows that she doesn't know what she wants.\",\n  'Sin duda, esta mujer sabe que no sabe lo que quiere.'],\n ['We have less than two hours.', 'Nos quedan menos de dos horas.'],\n ['The bus stopped, but nobody got off.',\n  'El bus se detuvo, pero nadie se bajó.'],\n ['Tom is a friend of mine.', 'Tom es un amigo mío.'],\n ['She told me she knew my brother.',\n  'Ella me dijo que conocía a mi hermano.'],\n ['Almost no one believed her.', 'Casi nadie le creyó.'],\n ['Will there be any food at the party?',\n  'Habrá algo de comida en la fiesta?'],\n ['You need to be more careful.', 'Debe ser más cuidadoso.'],\n ['She talks a lot.', 'Ella habla demasiado.'],\n [\"Why didn't Tom tell Mary?\", 'Por qué no se lo dijo Tom a Mary?'],\n ['You survived.', 'Sobrevivieron.'],\n [\"They're not mine.\", 'No son los míos.'],\n ['Thank you for your thorough explanation.',\n  'Gracias por tu meticulosa explicación.'],\n ['I do not have much time.', 'No tengo mucho tiempo.'],\n ['She deserved it.', 'Ella se lo merecía.'],\n [\"We're a bit busy at the moment. Can you hang on a minute?\",\n  'Ahora mismo estamos un poco ocupados, puede esperar un minuto?'],\n ['I wake him at six every morning.',\n  'Yo le levanto todos los días a las seis.'],\n [\"I'll go provided you go with me.\",\n  'Iré a condición de que usted vaya conmigo.'],\n [\"Speak louder. Your grandfather's hearing isn't so good.\",\n  'Habla más fuerte, la audición de tu abuelo no es muy buena.'],\n [\"We don't often eat out.\", 'No salimos a comer a menudo.'],\n ['He licked his fingers.', 'Él se chupó los dedos.'],\n ['I think the eggs that I just ate were rotten.',\n  'Creo que los huevos que acabo de comer estaban podridos.'],\n ['The tourists were ripped off at the nightclub.',\n  'Los turistas fueron timados en la discoteca.'],\n ['She likes to listen to music.', 'A ella le gusta escuchar música.'],\n ['How do you go to school every day?', 'Cómo vas a la escuela cada día?'],\n [\"Now that you're my girlfriend, I'm happy.\",\n  'Estoy feliz ahora que eres mi novia.'],\n [\"Don't give me that look.\", 'No me mires así.'],\n ['I have no one to help me.', 'No tengo a nadie que me ayude.'],\n ['Tom thought it was strange.', 'Tom pensó que era extraño.'],\n [\"Let's meet for a chat.\", 'Juntémonos para charlar.'],\n ['There are many foreign tourists in Asakusa.',\n  'Hay muchos turistas extranjeros en Asakusa.'],\n ['Raise your hands.', 'Levanta tus manos.'],\n ['Stop right here.', 'Párate aquí mismo.'],\n ['Communism was the biggest issue in the campaign.',\n  'El comunismo fue el mayor asunto en la campaña.'],\n ['I go to school at eight in the morning.',\n  'Voy al colegio a las ocho de la mañana.'],\n ['Tom almost laughed.', 'Tom casi se rio.'],\n ['These shoes are old, but I still like them.',\n  'Estos zapatos están viejos, pero todavía me gustan.'],\n ['What time do you want to meet?', 'A qué hora te quieres encontrar?'],\n ['I want to be a better person.', 'Quiero ser una mejor persona.'],\n ['I brought you some food.', 'Te traje comida.'],\n ['Tom was the last one to leave the party.',\n  'Tom fue el último en irse la fiesta.'],\n ['What time will you be back?', 'A qué hora volverás?'],\n [\"I really don't like goat cheese at all.\",\n  'No me gusta, para nada, el queso de cabra.'],\n ['He refused to sign the documents.', 'Él rehusó firmar los documentos.'],\n ['Long hair is out of fashion.', 'El pelo largo pasó de moda.'],\n ['I like tennis.', 'Me gusta el tenis.'],\n [\"It's true that he is in love with her.\",\n  'Es verdad que él está enamorado de ella.'],\n ['I read the book from beginning to end.',\n  'Leí el libro desde el principio hasta el final.'],\n [\"We're studying physical science.\", 'Estamos estudiando ciencia física.'],\n ['My grandmother lived to be ninety-five years old.',\n  'Mi abuela vivió hasta los noventa y cinco.'],\n ['She disappeared in the dark.', 'Ella desapareció en la oscuridad.'],\n ['Tom kissed Mary on her forehead.', 'Tom besó a Mary en la frente.'],\n ['I have plenty of things to eat in the pantry.',\n  'Tengo muchas cosas para comer en la despensa.'],\n ['He was kind enough to help me with my homework.',\n  'Él fue tan amable de ayudarme con mi tarea.'],\n ['All you do is complain!', 'Todo lo que haces es quejarte!'],\n [\"There's nowhere to hide.\", 'No hay donde esconderse.'],\n [\"Don't phone me when I'm at the office.\",\n  'No me telefonees cuando estoy en la oficina.'],\n ['Let me help.', 'Déjame ayudar.'],\n ['Do you know how to use a computer?', 'Sabes usar un ordenador?'],\n ['The pond dried up last summer.', 'La laguna se secó el verano pasado.'],\n [\"We're married to each other.\", 'Estamos casados.'],\n [\"That's not how you spell my name.\", 'Así no es como deletreas mi nombre.'],\n [\"That's perfect.\", 'Eso es perfecto.'],\n ['Ask him for advice.', 'Pídele consejo a él.'],\n ['He went to bed at eleven last night.',\n  'Ayer se fue a la cama a las once de la noche.'],\n [\"I'm very modest.\", 'Soy muy modesto.'],\n [\"I'm afraid of dogs.\", 'Le tengo miedo a los perros.'],\n ['This is the place where Tom was born.',\n  'Este es el lugar donde nació Tom.'],\n ['Tom lived in a small fishing village.',\n  'Tomás vivía en una pequeña aldea de pescadores.'],\n [\"I thought I'd always be alone.\", 'Pensé que siempre estaría solo.'],\n [\"What's the airmail rate?\", 'Cuál es la tasa para el correo aéreo?'],\n ['Many a mother spoils her sons by not being strict enough.',\n  'Muchas madres malcrían a sus hijos por no ser lo suficientemente estrictas.'],\n ['He got off the bus.', 'Él se bajó del autobús.'],\n [\"The medicine she took cured her of the bad cough she'd been suffering from.\",\n  'La medicina que tomó ella la curó de la tos de la que había estado sufriendo.'],\n ['You will get well in a week or so.',\n  'Te mejorarás en más o menos una semana.'],\n ['I am dying to see her again.', 'Me muero por volver a verla.'],\n ['Tom managed a small bar near Boston for quite a few years.',\n  'Tom ha llevado un pequeño bar cerca de Boston ya por varios años.'],\n ['Is one thousand yen enough?', 'Mil yenes son suficientes?'],\n ['His old cat is still alive.', 'Su viejo gato todavía está vivo.'],\n [\"We don't have time.\", 'No tenemos tiempo.'],\n [\"I'll be there.\", 'Estaré ahí.'],\n ['Did you come to town?', 'Viniste a la ciudad?'],\n ['Why is this happening?', 'Por qué está pasando esto?'],\n ['Tom plugged in his computer.', 'Tom enchufó su computador.'],\n ['Nobody told me what time I should come.',\n  'Nadie me dijo cuándo tenía que venir.'],\n [\"I'm crazy.\", 'Estoy loco.'],\n [\"It shouldn't happen again.\", 'No debería volver a pasar.'],\n [\"They're right behind you.\", 'Están justo detrás de vosotros.'],\n ['She took a deep breath.', 'Ella dio un respiro profundo.'],\n ['Do we have enough silverware for thirty people?',\n  'Tenemos suficientes cubiertos para treinta personas?'],\n ['Tom is hyperventilating.', 'Tom está hiperventilando.'],\n ['Everyone looks worried.', 'Todos lucen preocupados.'],\n ['These glasses are cool.', 'Estas gafas son geniales.'],\n ['There was no other choice but to abandon the entire project.',\n  'No hubo más opción, que renunciar al proyecto completo.'],\n [\"It's an interesting argument.\", 'Es un argumento interesante.'],\n [\"That's cruel.\", 'Eso es cruel.'],\n ['I watch television in the evening.', 'Yo veo televisión por la noche.'],\n ['Do you play the piano?', 'Toca el piano?'],\n [\"I'm here to save you.\", 'Estoy aquí para salvarte.'],\n [\"We're not eating.\", 'No estamos comiendo.'],\n ['Send this message to as many people as you can.',\n  'Envía este mensaje al mayor número de personas posible.'],\n ['She drew out the money from the bank.', 'Sacó el dinero del banco.'],\n [\"Don't let anyone leave this building.\",\n  'No dejes salir a nadie de este edificio.'],\n ['Tom talked Mary into inviting John to the party.',\n  'Tom convenció a Mary para que invitase a John a la fiesta.'],\n ['I see a star.', 'Veo una estrella.'],\n ['He finally fulfilled my request.', 'Al fin cumplió mi petición.'],\n ['I visited many parts of England.', 'Visité muchas partes de Inglaterra.'],\n ['He deserves more.', 'Él merece más.'],\n ['I apologized.', 'Me disculpé.'],\n ['Those are not your chairs.', 'Ésas no son tus sillas.'],\n [\"He's a DJ.\", 'Él es DJ.'],\n ['I am divorced.', 'Estoy divorciada.'],\n ['Keep an eye on Tom.', 'Vigilá a Tom.'],\n ['How many pens are there on the desk?',\n  'Cuántas biromes hay en el escritorio?'],\n [\"They didn't seem to notice it.\", 'No parecieron darse cuenta.'],\n ['My daugther wants a kitten.', 'Mi hija quiere un gatito.'],\n ['Pay your fare here.', 'Paga tu billete aquí.'],\n [\"Don't bite the hand that feeds you.\",\n  'No muerdas la mano que te alimenta.'],\n ['My roommate is too talkative.',\n  'Mi compañero de habitación es demasiado parlanchín.'],\n ['Everyone dies eventually.', 'Al final, todo el mundo se muere.'],\n [\"What's wrong with you?\", 'A ti qué te pasa?'],\n ['I want you to help us find out who killed Tom.',\n  'Quiero que nos ayudes a descubrir quién mató a Tom.'],\n ['Tom likes animals.', 'A Tom le gustan los animales.'],\n ['We should obey the traffic rules.',\n  'Debemos obedecer las leyes de tránsito.'],\n [\"Fiction is obliged to stick to possibilities. Truth isn't.\",\n  'La ficción está obligada a apegarse a las posibilidades. La verdad no.'],\n ['I feel helpless.', 'Me siento desamparado.'],\n ['I have a pain in my foot.', 'Me duele el pie.'],\n [\"A gun won't do you much good if you're not willing to shoot it.\",\n  'Una pistola no te servirá de mucho si no estás dispuesto a dispararla.'],\n ['I am going to go to America next year.', 'El año que viene iré a América.'],\n ['Will this be painful?', 'Esto será doloroso?'],\n ['Where should we meet?', 'Dónde deberiamos encontrarnos?'],\n [\"Tom's youngest daughter is his favorite.\",\n  'La hija pequeña de Tom es su favorita.'],\n ['If I had time, I would study French.',\n  'Si tuviera tiempo aprendería francés.'],\n ['When can I visit you?', 'Cuándo puedo visitarlos?'],\n ['He is still angry.', 'Él aún está enojado.'],\n ['Give me a second.', 'Dame un segundo.'],\n ['How many pencils do you have?', 'Cuántos lápices tiene usted?'],\n [\"I've done it before.\", 'Ya lo he hecho antes.'],\n ['Tom and Mary went deer hunting.', 'Tom y Mary fueron a cazar ciervos.'],\n ['I will start tonight.', 'Voy a empezar esta noche.'],\n ['Students may not enter the faculty lounge.',\n  'Los estudiantes no pueden entrar en la sala de profesores.'],\n [\"Tom didn't pull the trigger.\", 'Tom no apretó el gatillo.'],\n ['What happened to you? You look miserable.',\n  'Qué te sucedió? Luces miserable.'],\n ['She drinks like a fish.', 'Ella chupa como una esponja.'],\n ['Shall I prepare you a warm meal?', 'Te preparo algo caliente para tomar?'],\n ['My father bought me a camera for my birthday.',\n  'Mi padre me ha comprado una cámara de fotos por mi cumpleaños.'],\n [\"His advice didn't help much.\", 'Su consejo no sirvió de mucho.'],\n ['What are you doing on your night off?',\n  'Qué vas a hacer en tu noche libre?'],\n ['Whoever wins the race will receive the prize.',\n  'Quienquiera que gane la carrera obtendrá el premio.'],\n [\"You're the most handsome man I've ever seen.\",\n  'Eres el hombre más guapo que he visto en mi vida.'],\n ['My passport was stolen.', 'Me han robado el pasaporte.'],\n ['That guy is totally nuts!', 'Ese sujeto está completamente loco!'],\n ['I just arrived now.', 'Acabo de llegar.'],\n ['I hate my boss.', 'Odio a mi jefe.'],\n ['It was my first night among strangers.',\n  'Fue mi primera noche entre extranjeros.'],\n ['Tom agreed.', 'Tom convino aquiescente.'],\n ['He went away without saying a word.', 'Salió sin decir una palabra.'],\n ['I guess you will be very busy tonight.',\n  'Supongo que esta noche estarás muy ocupado.'],\n ['He is afraid of making mistakes.', 'Él teme equivocarse.'],\n ['Tom wondered what Mary was doing here.',\n  'Tom se preguntaba qué hacía María aquí.'],\n [\"We'll hide it.\", 'Nosotros lo esconderemos.'],\n [\"He apologized for his rudeness, but she wouldn't forgive him.\",\n  'Él pidió disculpas por su conducta, pero ella no lo perdonaría.'],\n ['A cargo vessel, bound for Athens, sank in the Mediterranean without a trace.',\n  'Un buque de carga, con rumbo a Atenas, se hundió en el Mediterráneo sin dejar rastro.'],\n ['When the man saw a policeman, he fled.',\n  'Cuando el hombre vio a un policía, huyó.'],\n ['We have other things to do.', 'Tenemos otras cosas que hacer.'],\n ['Where did you hide it?', 'Dónde lo escondiste?'],\n ['I thanked Mary for her help.', 'Agradecí a Mary por su ayuda.'],\n ['You might get injured.', 'Tú podrías resultar lastimado.'],\n ['Terrorists blew up a bus.', 'Los terroristas hicieron estallar un bus.'],\n ['He could not speak French well.', 'Él no podía hablar bien francés.'],\n ['He stopped playing baseball last season.',\n  'Él dejó el béisbol la temporada pasada.'],\n ['We have to use the stairs, because the elevator is being repaired.',\n  'Tenemos que usar la escalera, porque el ascensor está en reparación.'],\n ['Our school did away with uniforms last year.',\n  'Nuestro colegio abandonó los uniformes el año pasado.'],\n ['This antique clock is worth one thousand dollars.',\n  'Este antiguo reloj vale mil dólares.'],\n ['That really bothers me.', 'Eso realmente me molesta.'],\n ['We hardly ever see you around here anymore.',\n  'Casi no te vemos más por acá.'],\n ['She belongs to the Democratic Party.',\n  'Ella milita en el Partido Democrático'],\n ['I looked around.', 'Miré a mi alrededor.'],\n [\"Let's make it brief.\", 'Hagámoslo breve.'],\n ['Do you have a house?', 'Tienes una casa?'],\n ['Doctors use medical equipment.', 'Los doctores ocupan equipo médico.'],\n [\"These are Tom's ski boots.\", 'Estas son las botas de esquí de Tom.'],\n [\"I'd appreciate it if you would turn out the lights.\",\n  'Agradecería que apagaras la luz.'],\n ['Put the broom in the closet.', 'Pon la escoba en el armario.'],\n ['What do you want me to do?', 'Qué quiere que haga?'],\n ['I would like to offer you the position.',\n  'Me gustaría ofrecerte el puesto.'],\n [\"I can't put up with his violence any longer.\",\n  'No puedo soportar más su violencia.'],\n [\"It's only the beginning.\", 'Sólo es el principio.'],\n ['I really got depressed.', 'Me he deprimido profundamente.'],\n ['I use it every day.', 'Lo uso todos los días.'],\n ['She is incapable of deceit.', 'Ella es incapaz de engañar.'],\n ['I threw away my shoes.', 'Me deshice de mis zapatos.'],\n [\"I didn't get you a present.\", 'Yo no te di un regalo.'],\n ['I was overconfident.', 'Era arrogante.'],\n [\"Tom is very dependable, isn't he?\", 'Tom parece de fiar, eh?'],\n ['The sun is shining brightly.', 'El sol está brillando intensamente.'],\n ['He does not have any friends.', 'Él no tiene amigos.'],\n [\"I'll eat here.\", 'Comeré aquí.'],\n ['Keep on smiling.', 'Sigue sonriendo.'],\n ['Someone is standing behind the bushes taking pictures of us.',\n  'Alguien está parado atrás del arbusto sacándonos fotos.'],\n ['Playing tennis is fun.', 'Jugar tenis es divertido.'],\n ['Tom never seems to get upset.', 'Tom parece que nunca se enfada.'],\n ['The furniture was dusty.', 'El mobiliario tenía polvo.'],\n [\"I don't know anybody in this town.\",\n  'Yo no conozco a nadie en esta ciudad.'],\n ['I bought her a watch.', 'Le compré un reloj.'],\n [\"Let's go swimming.\", 'Vayamos a nadar.'],\n ['OK, listen up.', 'Vale, atendedme.'],\n ['The teacher intervened in the quarrel between the two students.',\n  'El profesor intervino en la riña entre los dos estudiantes.'],\n ['That was a close call.', 'Qué poco ha faltado.'],\n ['Everyone remarked on his new hairstyle.',\n  'Todo el mundo comentó sobre su nuevo peinado.'],\n [\"He doesn't have a job. He's retired.\",\n  'Él no tiene empleo. Está jubilado.'],\n ['I like him a lot, but sometimes he gets on my nerves.',\n  'Ella me agrada mucho, pero a veces me saca de las casillas.'],\n ['Old people are usually very wise.',\n  'La gente anciana es usualmente muy sabia.'],\n [\"Don't you think it a bad thing?\", 'No crees que es una cosa mala?'],\n ['I feel like doing something different today.',\n  'Tengo ganas de hacer algo diferente hoy.'],\n ['I had no trouble finding his office.', 'No me costó encontrar su oficina.'],\n ['Tom kept trying to call Mary, but she never answered her phone.',\n  'Tom siguió intentando llamar a Mary, pero ella nunca contestó su teléfono.'],\n ['The patient fainted at the sight of blood.',\n  'El paciente se desmayó al ver la sangre.'],\n ['This is how we cook rice.', 'Así es como cocinamos arroz.'],\n [\"I'm selling my car at a loss.\", 'Estoy vendiendo mi coche con pérdida.'],\n ['Take a few days off.', 'Toma unos días libres.'],\n ['I love French movies.', 'Me encantan las películas francesas.'],\n ['Tom raised his hand.', 'Tom alzó su mano.'],\n ['Tom congratulated Mary for her driving test.',\n  'Tom felicitó a Mary por su prueba de conducir.'],\n ['Ask him when the next plane leaves.',\n  'Pregúntele cuándo sale el próximo avión.'],\n ['What time does the club open?', 'A qué hora abre el club?'],\n ['The judge sentenced him to one year in prison.',\n  'El juez lo sentenció a un año de prisión.'],\n [\"He's wearing a white cotton shirt.\", 'Lleva una camisa blanca de algodón.'],\n ['He was very worried about having to spend Christmas in the hospital.',\n  'Él estaba muy preocupado por tener que pasar la Navidad en el hospital.'],\n ['I thought you were going to kill me.', 'Pensé que me ibas a matar.'],\n ['Cookie is my dog.', 'Cookie es mi perro.'],\n ['Is this correct?', 'Eso es correcto?'],\n ['I just want you to be involved.', 'Sólo quiero que estés implicado.'],\n ['What was the result?', 'Cuál fue el resultado?'],\n [\"Don't think I didn't try.\", 'No pienses que no lo intenté.'],\n ['I hate it.', 'Me la baja.'],\n ['I will never make that mistake again.', 'Nunca más cometeré ese error.'],\n ['My son gets on very well at school.',\n  'A mi hijo le va muy bien en el colegio.'],\n ['How well can you skate?', 'Qué tan bien sabes patinar?'],\n [\"He's not all there.\", 'No le llega agua al tanque.'],\n ['My dream is to be a firefighter.', 'Mi sueño es ser bombera.'],\n ['The storm let up.', 'La tormenta aflojó.'],\n ['Get to bed.', 'Vete a la cama.'],\n ['I already told you it was an accident.',\n  'Ya te dije que fue un accidente.'],\n [\"Tom's car is in the garage.\", 'El auto de Tomás está en el garage.'],\n ['He died two hours later.', 'Murió dos horas después.'],\n [\"I'll tell you what happened.\", 'Te diré lo que sucedió.'],\n ['The box is heavy.', 'La caja es pesada.'],\n ['Please come and see me if you have time.',\n  'Por favor ven a verme cuando tengas tiempo.'],\n ['They bought a few pieces of furniture when they got married.',\n  'Ellos compraron algunos muebles cuando se casaron.'],\n [\"I think I'm in trouble.\", 'Creo que tengo problemas.'],\n ['Tom felt uneasy talking to Mary about that matter.',\n  'Tom se sentía incomodo de hablar con Mary acerca de ese asunto.'],\n ['That stove smokes too much.', 'Esa estufa echa mucho humo.'],\n [\"I didn't know how to respond.\", 'No sabía cómo responder.'],\n ['He died of old age two years ago.',\n  'Él murió de edad hace dos años atrás.'],\n ['His car has just been repaired.', 'Su vehículo ha sido reparado recién.'],\n [\"If you don't study harder, you'll definitely fail.\",\n  'Si no estudias más, no cabe duda de que suspenderás.'],\n ['He asked me to read 5 poems.', 'Me pidió que leyera 5 poemas.'],\n ['What have you told them?', 'Qué le has dicho?'],\n [\"What if I'm right?\", 'Y si llevo razón?'],\n ['Tom was convicted and sentenced to death.',\n  'Tom fue declarado culpable y sentenciado a muerte.'],\n [\"I don't want to do this anymore.\", 'Ya no quiero seguir haciendo esto.'],\n ['I do the laundry on Sundays.', 'Yo lavo la ropa los domingos.'],\n [\"It's best to wear a cap on your head during the cold Moscow winters.\",\n  'Lo mejor es traer puesta una gorra en la cabeza durante los inviernos fríos de Moscú.'],\n ['I could feel nothing but the knife as it plunged into my back.',\n  'Yo solo pude sentir cómo se hundía el cuchillo por mi espalda.'],\n [\"I'm sorry, but I can't go with you.\",\n  'Lo siento, pero no puedo ir contigo.'],\n ['How rude of you!', 'Qué grosero!'],\n ['He has a nice income.', 'Él tiene un buen ingreso.'],\n ['That shirt looks good on you.', 'Esa camisa te queda bien.'],\n ['You did your best.', 'Diste lo mejor.'],\n [\"Are you absolutely sure you want to sell your father's guitar?\",\n  'Estás absolutamente seguro de que quieres vender la guitarra de tu padre?'],\n ['I jog twice a week.', 'Yo troto dos veces a la semana.'],\n [\"I'm fed up with your constant complaining.\",\n  'Estoy harto de tus constantes quejas.'],\n ['It would be best if I met him in person.',\n  'Sería mejor si yo le conociera en persona.'],\n ['That is the shop where I used to work.',\n  'Ésta es la tienda donde yo trabajaba.'],\n [\"Why didn't you try the dress on before you bought it?\",\n  'Por qué no te probaste el vestido antes de comprarlo?'],\n ['It was raining heavily in Osaka.', 'Llovía muy fuerte en Osaka.'],\n ['What does she look like?', 'Qué aspecto tiene?'],\n [\"I haven't made up my mind.\", 'No he tomado ninguna decisión.'],\n ['That we are not able to do.', 'Eso no podemos hacerlo.'],\n ['My dream is to become a famous singer.',\n  'Mi sueño es convertirme en una cantante famosa.'],\n ['Please lend me this book for a few days.',\n  'Por favor, préstame este libro por unos pocos días.'],\n ['Prices rose drastically as a result of this policy.',\n  'Los precios se elevaron drásticamente como resultado de su política.'],\n ['Where did you buy this guitar?', 'Dónde compraste esta guitarra?'],\n [\"Don't let go.\", 'No te sueltes.'],\n [\"I didn't mean to hit him.\", 'No fue mi intención golpearlo.'],\n ['Did Tom tell you anything interesting?', 'Tom te dijo algo interesante?'],\n ['Customers stopped coming to our shop.',\n  'Los clientes dejaron de venir a nuestra tienda.'],\n ['Do you still love Tom?', 'Aún amas a Tom?'],\n ['He expressed himself clearly.', 'Se expresó con claridad.'],\n ['Tom noticed that Mary was limping.', 'Tom notó que Mary estaba cojeando.'],\n ['Just come up here.', 'Limítate a venir aquí.'],\n [\"Tom can't take less.\", 'Tom no puede aceptar menos.'],\n ['Use it or lose it.', 'Úsalo o piérdelo.'],\n ['They sat by the fireplace.', 'Se sentaron junto a la chimenea.'],\n [\"Tom didn't know how to treat his employees right.\",\n  'Tom no supo comportarse correctamente con sus empleados.'],\n ['Tom told Mary the bad news.', 'Tom le dijo a Mary la mala noticia.'],\n ['I am a student in a university.', 'Soy un estudiante en una universidad.'],\n ['I want to know who killed Tom.', 'Quiero saber quién mató a Tom.'],\n ['The machine operates around the clock.', 'La máquina opera a toda hora.'],\n ['What a good scholar the author must be to write such a splendid book!',\n  'Pero qué erudito debe ser el autor como para escribir un libro tan espléndido!'],\n ['The policeman separated the two men who were fighting.',\n  'El policía separó a los dos hombres que estaban peleando.'],\n ['Why are you so sure Tom is Canadian?',\n  'Por qué estás tan seguro de que Tom es canadiense?'],\n ['A century is one hundred years.', 'Un siglo tiene cien años.'],\n ['Tom lay awake for a long time thinking about Mary.',\n  'Tom yació despierto durante un largo rato pensando en Mary.'],\n [\"I'm impartial.\", 'Soy imparcial.'],\n ['The frescoes of the cathedral are very interesting.',\n  'Los frescos de la catedral son muy interesantes.'],\n ['My book is here.', 'Mi libro está aquí.'],\n [\"There's a soccer match tomorrow.\", 'Hay un partido de fútbol mañana.'],\n ['The influence of TV on society is great.',\n  'La influencia de la televisión sobre la sociedad es grande.'],\n ['Do you want to go to the zoo?', 'Quieres ir al zoo?'],\n ['Tom first met Mary when they were in high school.',\n  'Tom conoció a Mary cuando iban en enseñanza media.'],\n ['My brother is a vet.', 'Mi hermano es veterinario.'],\n ['I think Tom is going to die.', 'Creo que Tom se va a morir.'],\n [\"I don't believe Tom can do that.\", 'No creo que Tom pueda hacer eso.'],\n ['What did Tom tell Mary not to do?',\n  'Qué le dijo Tom a Mary que no hiciera?'],\n ['The mall is deserted.', 'El centro comercial está desierto.'],\n ['What do you believe?', 'Qué creés?'],\n ['She said she was ill in bed, which was a lie.',\n  'Ella dijo que estaba en cama enferma, lo que era mentira.'],\n ['I want to be rich.', 'Quiero ser rica.'],\n ['What train you are going to take?', 'Qué tren vas a coger?'],\n ['The floor was very cold.', 'El piso estaba muy frío.'],\n [\"Please don't use English.\", 'Por favor no uses inglés.'],\n ['Tom is very smart, just like you.', 'Tom es muy listo, justo como tú.'],\n ['I have no intention of getting wet.', 'No tengo intención de mojarme.'],\n [\"The man's behavior was very odd.\",\n  'El comportamiento del hombre era muy extraño.'],\n [\"Tom and I could've saved ourselves a lot of time by just doing that ourselves.\",\n  'Tom y yo podríamos habernos ahorrado un montón de tiempo simplemente haciéndolo nosotros.'],\n ['There was a striking resemblance between them.',\n  'Había un parecido sorprendente entre ellos.'],\n [\"That man's not to be trusted.\", 'Ese hombre no es de fiar.'],\n ['Besides being a doctor, he was a very famous novelist.',\n  'Además de ser un doctor, también fue un muy famoso novelista.'],\n [\"To tell the truth, I didn't do my homework.\",\n  'A decir verdad, no hice mi tarea.'],\n ['They sat down.', 'Se sentaron.'],\n ['He had a traffic accident on his way to school.',\n  'Él tuvo un accidente de tráfico de camino al colegio.'],\n ['The desire to fly in the sky like a bird inspired the invention of the airplane.',\n  'El deseo de volar como un pájaro inspiró la invención del avión.'],\n ['That typhoon prevented me from going out.', 'El tifón me impidió salir.'],\n [\"I couldn't resist the urge to applaud.\",\n  'No pude resistir el impulso de aplaudir.'],\n ['Are you done with your homework yet?', 'Ya terminaste tu tarea?'],\n [\"I'm getting undressed.\", 'Me estoy desvistiendo.'],\n ['The story is full of holes.', 'La historia está llena de hoyos.'],\n ['I gave my books to those people.', 'Le di mis libros a esa gente.'],\n [\"We're even.\", 'Estamos a mano.'],\n [\"He's leaving for China tomorrow.\", 'Él se va a China mañana.'],\n ['I often watch TV before dinner.', 'A menudo veo la tele antes de cenar.'],\n ['Try us again next Monday.', 'Inténtelo de nuevo el lunes.'],\n ['She was depressed by all her problems.',\n  'Ella estaba deprimida por todos sus problemas.'],\n ['The boy is afraid of the dark.', 'Al niño le da miedo la oscuridad.'],\n ['My mother loves me.', 'Mi madre me ama.'],\n ['Tom disliked school when he was younger.',\n  'A Tom no le gustaba el colegio cuando era más joven.'],\n ['Will you let me use your telephone, please?',\n  'Me dejarías usar tu teléfono, por favor?'],\n ['Give me your sandwich.', 'Dame tu sándwich.'],\n ['He promised not to smoke.', 'Él me prometió no fumar.'],\n ['Those are the risks.', 'Esos son los riesgos.'],\n ['No one voted against it.', 'Nadie votó en contra.'],\n ['My life was in danger.', 'Mi vida estaba en peligro.'],\n ['The police have caught him.', 'Le ha cogido la policía.'],\n ['She taught me how to make a web site.',\n  'Ella me enseñó a hacer una página en Internet.'],\n ['The more a man knows, the more he discovers his ignorance.',\n  'Cuanto más sabe un hombre, más descubre su ignorancia.'],\n ['There are four people in my family.',\n  'Mi familia está compuesta por cuatro personas.'],\n ['Babies cry when they are hungry.',\n  'Los bebés lloran cuando tienen hambre.'],\n [\"It's incredible!\", 'Es increíble!'],\n ['John met Mary on his way to school.',\n  'John se juntó con Mary de camino a la escuela.'],\n [\"The child slept on its mother's lap.\",\n  'El niño durmió en el regazo de su madre.'],\n [\"I don't like visiting big cities.\",\n  'No me gusta visitar ciudades grandes.'],\n ['This is the umbrella I bought yesterday.',\n  'Éste es el paraguas que compré ayer.'],\n ['I really like the concept of this website.',\n  'Realmente me agrada el concepto de este sitio web.'],\n ['Go back to work.', 'Vuelve al trabajo.'],\n ['Tom was the sort of man you could get along with.',\n  'Tom era el tipo de hombre con el que te podías llevar bien.'],\n ['Which one are you going to use?', 'Cuál vas a utilizar?'],\n [\"Do you think I'm stupid?\", 'Pensáis que soy idiota?'],\n ['Tom showed me a magic trick.', 'Tom me enseñó un truco de magia.'],\n ['She was wearing an ugly dress.', 'Llevaba puesto un vestido feo.'],\n ['His ideas are too radical to be acceptable to most people.',\n  'Sus ideas son demasiado radicales para que les resulten aceptables a la mayoría de la gente.'],\n ['You should cut down on the amount of fattening food that you eat.',\n  'Deberías reducir la cantidad de grasas que comes.'],\n ['It is clear that he is guilty.', 'Está claro que es culpable.'],\n ['You need to shut up.', 'Tienes que callarte.'],\n ['She arrived just as I was leaving.',\n  'Ella llegó justo cuando yo me estaba yendo.'],\n ['She does not have much money.', 'Ella no tiene mucho dinero.'],\n [\"We're doing fine.\", 'Lo llevamos bien.'],\n [\"The movie wasn't as interesting as the book.\",\n  'La película no era tan interesante como el libro.'],\n [\"This clock isn't working.\", 'Este reloj no funciona.'],\n ['I ate a hot dog for lunch.', 'Tomé un perrito caliente para comer.'],\n [\"Don't forget about me.\", 'No te olvides de mí.'],\n ['The house is on fire.', 'La casa se está quemando.'],\n [\"I don't understand French at all.\", 'No entiendo nada de francés.'],\n ['The gasoline truck ran into the gate and blew up.',\n  'El camión de combustible se estrelló contra la puerta y estalló.'],\n ['He has to work hard in order to support his family.',\n  'Él tiene que trabajar mucho para mantener a su familia.'],\n ['Will you tell me the truth?', 'Me dirá la verdad?'],\n ['Keep warm.', 'Mantenete caliente.'],\n ['His grandfather is what is called a self-made man.',\n  'Mi abuelo es lo que se dice un hombre hecho a si mismo.'],\n ['I got carded.', 'Me pidieron identificarme.'],\n ['He confessed all his sins.', 'Confesó todas sus culpas.'],\n [\"I'll go change my clothes.\", 'Voy a ir a cambiarme de ropa.'],\n ['Could you please take me back home?', 'Podrías llevarme de vuelta a casa?'],\n [\"Don't forget to call me.\", 'No olvides llamarme.'],\n ['The news made him happy.', 'La noticia le hizo feliz.'],\n ['The animals had to be killed.', 'Los animales debían ser matados.'],\n ['Illness forced him to give up school.',\n  'La enfermedad lo forzó a abandonar la escuela.'],\n ['He regretted not having taken my advice.',\n  'Él lamentó no haber seguido mi consejo.'],\n ['I think Tom drank out of my glass by mistake.',\n  'Creo que Tom se equivocó al beber de mi vaso.'],\n ['Why are you angry with me?', 'Por qué estás cabreada conmigo?'],\n ['Let the children play.', 'Deja que los niños jueguen.'],\n ['He bored us with his long stories.',\n  'Él nos aburrió con su larga historia.'],\n [\"I don't know about you, but I'm starved.\",\n  'No sé nada de vos, pero me muero de hambre.'],\n ['I have a lot of things to tell you.', 'Tengo muchas cosas que decirte.'],\n ['Boys are more aggressive than girls.',\n  'Los chicos son más agresivos que las chicas.'],\n ['He always borrows money from me.', 'Él siempre me pide dinero prestado.'],\n ['He wants to study music and dance.', 'Él quiere estudiar música y danza.'],\n ['They left.', 'Se iban.'],\n ['Are you a ghost?', 'Eres un fantasma?'],\n [\"Don't leave the windows open.\", 'No dejes abiertas las ventanas.'],\n ['Cut it out, Tom.', 'Déjalo ya, Tom.'],\n [\"I'm eating now.\", 'Ahora estoy comiendo.'],\n ['Tom stands by me whenever I am in trouble.',\n  'Tom me apoya siempre que tengo problemas.'],\n ['He earns over 500 dollars a month from that job.',\n  'Él gana más de 500 dólares al mes con ese trabajo.'],\n ['I told you to be careful.', 'Te dije que tuvieras cuidado.'],\n ['They never tell a lie.', 'Ellas no mienten nunca.'],\n ['Have you ever seen anything so beautiful?',\n  'Alguna vez viste algo tan lindo?'],\n ['What a shame!', 'Qué vergüenza!'],\n ['We need to chat soon.', 'Necesitamos charlar pronto.'],\n ['She introduced her sister to him more than two years ago.',\n  'Ella le presentó a su hermana hace más de dos años.'],\n [\"Didn't you hear the scream?\", 'No has oído ese grito?'],\n ['Tom was lost.', 'Tom estaba perdido.'],\n ['Are you 18?', 'Has cumplido dieciocho?'],\n ['I am a member of the basketball team.',\n  'Soy miembro del equipo de baloncesto.'],\n ['Tom is a diabetic.', 'Tom es diabético.'],\n ['I got out of the car at 40th Street.', 'Me bajé del auto en la calle 40.'],\n ['Sorry for the delay.', 'Perdón por la tardanza.'],\n ['Is English spoken in Canada?', 'Se habla inglés en Canadá?'],\n [\"Tom thought Mary wouldn't want to live in Boston.\",\n  'Tom pensó que Mary no querría vivir en Boston.'],\n ['Tom told me I should get myself a girlfriend.',\n  'Tom me dijo que debía conseguirme una novia.'],\n [\"You didn't invite me.\", 'No me invitaste.'],\n ['Does it hurt when you chew?', 'Te duele cuando masticás?'],\n [\"You can run, but you can't hide.\", 'Puedes correr, pero no esconderte.'],\n ['She acted in the play.', 'Ella actuó en la obra.'],\n ['I just want to hug you.', 'Solo quiero abrazarte.'],\n ['Tom leaned on his cane.', 'Tom se apoyó en su bastón.'],\n ['That makes no sense at all.', 'Eso no tiene ningún sentido.'],\n ['I want to buy this jacket.', 'Quiero comprar esta chaqueta.'],\n ['He deposited 100 dollars in his saving account.',\n  'Él ingresó 100 dólares en su cuenta de ahorros.'],\n [\"I'm not as healthy as I used to be.\", 'No estoy tan sano como antaño.'],\n ['Yes, I know it.', 'Sí, lo sé.'],\n ['I hope that you will get well soon.', 'Espero que pronto te pongas bien.'],\n ['Her house is close to the park.', 'Su casa está cerca del parque.'],\n ['About how many books do you have?', 'Cuántos libros tienes más o menos?'],\n ['I was at home almost all day yesterday.',\n  'Ayer estuve casi todo el día en casa.'],\n ['He is blinded by love.', 'Él está ciego de amor.'],\n ['He had few friends and little money.',\n  'Él tenía pocos amigos y poco dinero.'],\n ['He is a scientist who is respected by everybody.',\n  'Él es un científico respetado por todos.'],\n ['They say that she is in love with him.', 'Dicen que está enamorada de él.'],\n ['Tell her to not look for me.', 'Dile a ella que no me busque.'],\n [\"When a good opportunity presents itself, I don't let it pass.\",\n  'Cuando se presenta una oportunidad favorable, no la dejo pasar.'],\n ['She has a cold and is absent from school.',\n  'Ella está resfriada y no viene al colegio.'],\n ['The library is on the 4th floor.', 'La biblioteca está en el cuarto piso.'],\n [\"I haven't yet read all of these books.\",\n  'Todavía no he leído todos estos libros.'],\n [\"Mary doesn't have expensive tastes.\", 'María no tiene gustos caros.'],\n ['He has a lot of original ideas.', 'Él tiene muchas ideas originales.'],\n ['I explained the rules to her.', 'Le expliqué las reglas.'],\n ['We were to be married in May but had to postpone the marriage until June.',\n  'Nos íbamos a casar en mayo, pero tuvimos que posponer la boda hasta junio.'],\n ['The computer is to her left.', 'El ordenador está a su izquierda.'],\n ['He has a lot of money.', 'Él tiene mucho dinero.'],\n [\"I'll be back in two hours.\", 'Vuelvo en dos horas.'],\n ['It is better to live rich, than to die rich.',\n  'Es mejor vivir rico que morir rico.'],\n [\"There's a long line at every cash register.\",\n  'Hay una larga cola en todas las cajas registradoras.'],\n [\"I'm sure you're going to like this lunch.\",\n  'Estoy seguro de que les gustará el almuerzo.'],\n ['We jog before breakfast every morning.',\n  'Hacemos footing todas las mañanas antes del desayuno.'],\n ['Nobody laughed.', 'Nadie se rió.'],\n ['I have already finished reading this book.',\n  'Ya me he acabado este libro.'],\n [\"We haven't seen it yet.\", 'Todavía no lo hemos visto.'],\n [\"What's in that cupboard?\", 'Qué hay en ese aparador?'],\n ['Good for you.', 'Bien por ti.'],\n [\"Let's play something.\", 'Vamos a jugar.'],\n ['Next Monday is a holiday.', 'El próximo lunes es feriado.'],\n ['This is the only book I have.', 'Ese es el único libro que tengo.'],\n ['Is he aware of the difficulty?', 'Está él al tanto de la dificultad?'],\n ['I wanted us to be happy.', 'Quería que fuéramos felices.'],\n ['I believe in fate.', 'Creo en el destino.'],\n ['Tom saw his reflection in the mirror.', 'Tom vio su reflejo en el espejo.'],\n ['Everybody needs one.', 'Todos necesitan uno.'],\n ['Tom had no further questions.', 'Tom no tenía más preguntas.'],\n ['Will you please show me the way?',\n  'Podrías enseñarme el camino, por favor?'],\n ['I know a shortcut.', 'Yo conozco un atajo.'],\n ['What are those?', 'Qué son ésos?'],\n [\"We haven't got much time.\", 'No tenemos mucho tiempo.'],\n [\"It's a lot of fun to be with you.\", 'Es muy divertido estar contigo.'],\n ['Could you just get straight to the point?',\n  'Podrías ir directamente al grano?'],\n [\"That's the right answer.\", 'Esa es la respuesta correcta.'],\n ['Would you like to drink anything?', 'Te gustaría beber algo?'],\n ['Is this love?', 'Esto es amor?'],\n [\"She starts her job at seven o'clock.\",\n  'Ella empieza a trabajar a las siete.'],\n [\"Let's go to a movie.\", 'Vamos a ver una película.'],\n [\"He hasn't made a record or had a concert for many years.\",\n  'Él no ha hecho una grabación ni ha realizado un concierto por muchos años.'],\n ['You look tired.', 'Usted se ve cansado.'],\n [\"I'm looking for my brother.\", 'Estoy buscando a mi hermano.'],\n ['The kitten slept soundly.', 'El gatito dormía profundamente.'],\n ['She married him.', 'Ella se casó con él.'],\n ['He is the older of the two.', 'Él es el mayor de los dos.'],\n ['\"She likes music.\" \"So do I.\"',\n  '\"A ella le gusta la música.\" \"A mí también.\"'],\n ['If she were here now, I would tell her the truth.',\n  'Si ella estuviera aquí ahora, le diría la verdad.'],\n ['My shopping bag broke.', 'Mi bolsa de la compra se rompió.'],\n ['We were friends.', 'Éramos amigos.'],\n ['Everyone cheered.', 'Hubo un vitoreo general.'],\n ['These tips may save your life.',\n  'Estos consejos podrían salvarte la vida.'],\n ['Where are your friends?', 'Dónde están tus amigos?'],\n [\"Why shouldn't truth be stranger than fiction? Fiction, after all, has to make sense.\",\n  'Por qué no podría la verdad ser más extraña que la ficción? Después de todo, la ficción tiene que tener sentido.'],\n ['Come on, give it to me.', 'Venga, dámelo.'],\n ['Tom also plays the violin.', 'Tom también toca el violín.'],\n ['Tom admitted that he was afraid.', 'Tom confirmó sus temores.'],\n ['Tom is clearly worn out.', 'Tom está claramente agotado.'],\n ['The dogs bayed at the full moon.', 'Los perros aullaban a la luna llena.'],\n [\"That's obscene.\", 'Eso es obsceno.'],\n ['They went fishing yesterday.', 'Ayer se fueron de pesca.'],\n ['He took out his handkerchief.', 'Él se sacó el pañuelo.'],\n ['I was expecting you.', 'Te estuve esperando.'],\n ['What do you have on the menu today?', 'Qué tienes en el menú hoy?'],\n ['Tom made it quite clear what he wanted.',\n  'Tom dejó muy claro lo que él quería.'],\n ['This is really cool.', 'Esto es genial.'],\n ['She needs our help.', 'Necesita nuestra ayuda.'],\n ['She is out now.', 'Ella está afuera ahora.'],\n ['They abandoned their children in the forest.',\n  'Abandonaron a sus hijos en el bosque.'],\n ['Tom got to the airport just in the nick of time.',\n  'Tom llegó al aeropuerto justo en el último momento.'],\n [\"Tom said he couldn't clean the pool tomorrow afternoon.\",\n  'Tom dijo que no podría limpiar la piscina mañana por la tarde.'],\n ['Tom probably knows the answer.', 'Tom probablemente sepa la respuesta.'],\n [\"It's understood that we'll start tomorrow.\",\n  'Está entendido que empezaremos mañana.'],\n ['Hit Tom.', 'Golpea a Tom.'],\n ['I handed in my report yesterday.', 'Yo entregué mi reporte ayer.'],\n ['I have neglected you so long that I feel a bit shy in visiting you.',\n  'Te he dejado de lado por tanto tiempo que tengo un poco de vergüenza de visitarte.'],\n ['Give me a hammer.', 'Deme un martillo.'],\n ['I have a small fever.', 'Tengo un poco de fiebre.'],\n [\"It must've cost a fortune.\", 'Tiene que haber costado una fortuna.'],\n ['I lied.', 'Mentí.'],\n ['I could have sworn I saw somebody.', 'Juraría que vi a alguien.'],\n ['I suppose you like him.', 'Yo supongo que te gusta él.'],\n [\"I didn't know anyone at the party.\", 'No conocía a nadie en la fiesta.'],\n [\"You want a divorce, don't you?\", 'Quieres el divorcio, no es así?'],\n ['I am a teacher of English.', 'Soy maestra de inglés.'],\n ['You must be more polite.', 'Tenés que ser más respetuoso.'],\n ['Who knows?', 'Aabe...'],\n ['She asked me if I knew her address.',\n  'Ella me preguntó si conocía su dirección.'],\n ['I once lived in Rome.', 'Una vez viví en Roma.'],\n [\"I'll scold him.\", 'Le reñiré.'],\n ['Tom ate what little food he had left.',\n  'Tom se comió la poca comida que le quedaba.'],\n ['A policeman was gazing at a suspicious pedestrian.',\n  'Un policía estaba observando a un peatón sospechoso.'],\n ['I never looked for you.', 'Nunca te busqué.'],\n ['Tom once worked at a bakery.', 'Tom una vez trabajó en una panadería.'],\n [\"Everyone looked at Tom to see what he'd do.\",\n  'Todos miraron a Tom para ver qué haría.'],\n ['Tom was abused by his father.', 'Tom fue abusado por su padre.'],\n [\"Just a moment. I haven't made up my mind yet.\",\n  'Un momento. Aún no me decido.'],\n ['It was the biggest mistake of my life.',\n  'Fue el error más grande de mi vida.'],\n ['Tom is the one who feeds the dog.',\n  'Tom es el único que da de comer al perro.'],\n ['I will leave as soon as the bell rings.',\n  'Me iré tan pronto como suene la campana.'],\n ['Would you speak more slowly, please?',\n  'Podrías hablar más lento, por favor?'],\n ['I am not a doctor, but a teacher.', 'No soy médico, soy profesor.'],\n ['Neither of those boys can speak French.',\n  'Ninguno de esos chicos sabe hablar francés.'],\n [\"I'm learning English.\", 'Estoy aprendiendo inglés.'],\n ['Do you know of any good restaurants around here?',\n  'Conoces algún buen restaurante por aquí?'],\n ['His brother had been missing for a while.',\n  'Su hermano llevaba un tiempo desaparecido.'],\n ['I looked down.', 'Yo miré hacia abajo.'],\n ['Please close the drapes.', 'Por favor, cierra las cortinas.'],\n ['His breath smells like goat cheese.', 'Su aliento huele a queso de cabra.'],\n ['Nothing is happening.', 'No está pasando nada.'],\n [\"He didn't have enough money to ride home on the train.\",\n  'Él no tenía suficiente dinero para ir a casa en tren.'],\n ['I am delighted to be here.', 'Estoy encantado de estar aquí.'],\n [\"Let's start with leftovers.\", 'Empezaremos por las sobras.'],\n ['What time does the show start?', 'A qué hora empieza la función?'],\n ['The studio is very small, with no place to hide.',\n  'El estudio es muy pequeño, sin lugares donde esconderse.'],\n [\"I don't mind sleeping on the floor.\", 'No me importa dormir en el suelo.'],\n ['Tom never helps me.', 'Tom no me ayuda nunca.'],\n ['Only one little boy survived the traffic accident.',\n  'Solo un niño pequeño sobrevivió al accidente de tránsito.'],\n [\"Tom won't be here next month.\", 'Tom no estará acá el mes que viene.'],\n ['You should go to the barbershop.', 'Deberías ir a la barbería.'],\n ['Our car ran out of gas after ten minutes.',\n  'Nuestro auto se quedó sin combustible después de diez minutos.'],\n ['Whose bicycle is this?', 'De quién es esta bicicleta?'],\n ['He never stopped writing.', 'Él nunca dejó de escribir.'],\n ['She was restless because she did not have anything to do.',\n  'Ella estaba impaciente porque no tenía nada que hacer.'],\n ['Put it in first and slowly let off the clutch while giving it a little gas.',\n  'Mete primera y suelta lentamente el embrague mientras aceleras un poco.'],\n ['How would you translate this sentence?', 'Cómo traducirías esta oración?'],\n ['Do you eat in the classroom?', 'Comen en el aula?'],\n [\"I wish I'd studied French when I was younger.\",\n  'Me gustaría haber estudiado francés cuando era más joven.'],\n ['Your questions were too direct.', 'Tus preguntas fueron muy directas.'],\n ['Can you remember the first time we met each other?',\n  'Te puedes acordar de la primera vez que nos vimos el uno al otro?'],\n ['I have plenty of time to do that.', 'Tengo harto tiempo para hacer eso.'],\n ['I know Tom from work.', 'Conozco a Tom del trabajo.'],\n ['What have you done?', 'Qué hiciste?'],\n ['It is said the house is haunted.', 'Se dice que la casa está embrujada.'],\n ['Last year in the Philippines, earthquakes and tidal waves resulted in the deaths of more than 6,000 people.',\n  'El año pasado, los terremotos y los maremotos cobraron más de 6.000 vidas en Filipinas.'],\n [\"I'm very grateful to you.\", 'Te estoy muy agradecido.'],\n [\"Most people don't have a problem with that.\",\n  'La mayoría de la gente no tiene problema con eso.'],\n [\"I can't stand that noise.\", 'No puedo soportar ese ruido.'],\n ['Everything will change.', 'Todo cambiará.'],\n ['None of us is perfect.', 'Ninguno de nosotros es perfecto.'],\n [\"I'll go back to Boston.\", 'Me devolveré a Boston.'],\n [\"This museum isn't open on Mondays.\", 'Este museo no se abre los lunes.'],\n ['Does he know how you feel?', 'Sabe cómo te sientes?'],\n [\"They don't know that I'm Japanese.\", 'No saben que soy japonesa.'],\n ['He often goes off on wild goose chases.',\n  'Él a menudo se embarca en metas imposibles.'],\n [\"My father's hobby is fishing.\", 'El pasatiempo de mi padre es la pesca.'],\n ['I had stuff to do.', 'Tenía cosas que hacer.'],\n ['He was too drunk to remember to shut the back door.',\n  'Él iba borracho y olvidó cerrar la puerta trasera.'],\n ['I cut a branch from the tree.', 'He cortado una rama del árbol.'],\n ['How many days do we have left until summer vacation begins?',\n  'Cuántos días nos quedan para que empiecen las vacaciones de verano?'],\n [\"How's business?\", 'Cómo va el negocio?'],\n ['Close your mouth.', 'Cerrá la boca.'],\n [\"Tom doesn't want to do anything but swim.\", 'Tom sólo quiere nadar.'],\n ['All men are equal.', 'Los hombres son todos iguales.'],\n ['I have an electric guitar.', 'Tengo una guitarra eléctrica.'],\n ['Would you please stop singing so loudly? This is not a cheap drinking place.',\n  'Por favor, podrías dejar de cantar tan fuerte? Este no es un bar cualquiera.'],\n ['Which do you like better, the Giants or the Dragons?',\n  'Qué prefieres: los Gigantes o los Dragones?'],\n [\"I don't feel like playing tennis today.\",\n  'Hoy no tengo ganas de jugar al tenis.'],\n ['He looks well.', 'Tiene buen aspecto.'],\n ['Do you want to make some money today?',\n  'Quieres ganar algo de dinero hoy?'],\n ['He bowed to the Queen.', 'Él se inclinó ante la reina.'],\n [\"She says you'll bring some friends along.\",\n  'Ella dice que tú traerás algunos amigos contigo.'],\n [\"Tom's funny.\", 'Tom es alegre.'],\n ['Tom is taller and stronger than Mary.',\n  'Tom es más alto y más fuerte que Mary.'],\n ['He abandoned his family.', 'Él abandonó a su familia.'],\n ['Are there a lot of Moroccans in Germany?',\n  'Hay muchos marroquíes en Alemania?'],\n ['Every mistake made me stronger.', 'Cada error me fortalece.'],\n ['Did she show you the picture?', 'Ella te enseñó la foto?'],\n ['Look at yourself in the mirror.', 'Mirate en el espejo.'],\n ['I wish I could speak French like a native speaker.',\n  'Ojalá hablara francés como un nativo.'],\n ['Tom asked Mary about her new job in Boston.',\n  'Tom le preguntó a Mary por su nuevo trabajo en Boston.'],\n [\"Tom doesn't have to wash the car. Mary's already washed it.\",\n  'Tom no necesita lavar el auto. Mary ya lo lavó.'],\n ['It was yesterday.', 'Fue ayer.'],\n [\"What's your philosophy?\", 'Cuál es tu filosofía?'],\n [\"What's the spiciest thing you've ever eaten?\",\n  'Qué es lo más picante que has comido nunca?'],\n ['I love to teach.', 'Me encanta enseñar.'],\n [\"Tom is a friend of Mary's.\", 'Tom es un amigo de Mary.'],\n ['Who killed Tom?', 'Quién mató a Tom?'],\n [\"You're soaking wet.\", 'Estás empapado.'],\n ['You may park here.', 'Te puedes aparcar aquí.'],\n ['The children amused themselves by playing games.',\n  'Los niños se divertían jugando.'],\n ['What are you smirking at?', 'De qué te estás sonriendo?'],\n ['They know what happened.', 'Ellos saben qué pasó.'],\n ['Where can I buy a map?', 'Dónde puedo comprar un mapa?'],\n ['We go to church together.', 'Vamos juntos a la iglesia.'],\n ['I knew Tom would be there.', 'Sabía que Tom estaría allí.'],\n ['Tom and Mary know each other.', 'Tom y Mary se conocen.'],\n ['Will you pick out a tie for me?', 'Me eliges una corbata?'],\n [\"I knew I shouldn't have put off doing my homework until the last minute.\",\n  'Se que no debería haber pospuesto hacer los deberes hasta el último minuto.'],\n ['You are prohibited from smoking here.', 'Tu tienes prohibido fumar aquí.'],\n ['What are your intentions?', 'Cuáles son sus miras?'],\n ['I cannot excuse her.', 'No puedo perdonarla.'],\n ['Tom came back home after dark.',\n  'Tom volvió a casa después de que oscureciese.'],\n ['Tom is improving.', 'Tom va mejorando.'],\n ['Life is like riding a bicycle. To keep your balance you must keep moving.',\n  'La vida es como montar en bicicleta: para mantenerte en equilibrio tienes que seguir moviéndote.'],\n ['When the phone rang, he sprang out of bed.',\n  'Cuando sonó el teléfono, él saltó de la cama.'],\n ['Can you recommend a good game to me?',\n  'Me puede recomendar un buen juego?'],\n ['Tom has wavy brown hair and blue eyes.',\n  'Tom tiene pelo castaño ondulado y ojos azules.'],\n ['Tom is useless.', 'Tomás es un cero a la izquierda.'],\n ['When I met her the other day she asked of my parents.',\n  'Cuando me la encontré el otro día, me preguntó por mis padres.'],\n [\"Tom loved the color of Mary's new dress.\",\n  'A Tom le encantaba el color del nuevo vestido de Mary.'],\n ['You are responsible for the death of the child.',\n  'Tú eres responsable por la muerte del niño.'],\n ['Where is your cap?', 'Dónde está tu gorra?'],\n [\"I'm severely allergic to peanuts.\",\n  'Soy extremadamente alérgico a los cacahuetes.'],\n ['How many galaxies are there in the universe?',\n  'Cuántas galaxias hay en el universo?'],\n ['There was no choice but to sit and wait.',\n  'No había otra opción excepto sentarse y esperar.'],\n ['Tom asked Mary which way to turn.',\n  'Tom le preguntó a Mary hacia donde doblar.'],\n ['Her composition was free from mistakes.',\n  'Su redacción estaba libre de fallos.'],\n ['The store is closed until further notice.',\n  'La tienda está cerrada hasta próximo aviso.'],\n [\"I think I've broken my arm.\", 'Creo que me he roto el brazo.'],\n ['He has a mild nature.', 'Él tiene una personalidad dócil.'],\n ['Go first to those who you are sure will help you.',\n  'Acude primero a aquellos de los que estás seguro de que te ayudarán.'],\n [\"Now what's wrong?\", 'Qué ha pasado ahora?'],\n ['The village is connected with our town by a bridge.',\n  'El pueblo está unido con nuestra ciudad a través de un puente.'],\n ['The milk tasted sour.', 'La leche sabía agria.'],\n ['I confessed to stealing the money.', 'Confesé haber robado el dinero.'],\n [\"We're an hour behind.\", 'Llevamos una hora de retraso.'],\n ['This CD belongs to her.', 'Este CD le pertenece a ella.'],\n ['He was scared you would shoot him.',\n  'Él tenía miedo de que le dispararas.'],\n ['He has a dog.', 'Tiene un perro.'],\n ['She kept on talking while eating.',\n  'Ella siguió hablando en lo que comía.'],\n ['I came to talk to you.', 'Vine a hablar con vos.'],\n ['It took less than five minutes.', 'Tomó menos de cinco minutos.'],\n ['Everybody needs to know this.', 'Todos tienen que saber esto.'],\n ['I agree with him on that point.', 'Estoy de acuerdo con él en ese punto.'],\n ['We got in after a long wait.',\n  'Después de una larga espera pudimos entrar.'],\n [\"You've put on weight, haven't you?\", 'Has cogido peso, verdad?'],\n ['Could I park my car here?', 'Puedo aparcar mi coche aquí?'],\n ['How long is your Christmas vacation?',\n  'Cuánto tiempo de vacaciones tienes en Navidad?'],\n ['Tom had trouble resolving the situation.',\n  'Tom tuvo problemas para resolver la situación.'],\n ['The author of this book is still young.',\n  'El autor de este libro es aún joven.'],\n ['I have the ace of clubs.', 'Tengo el as de tréboles.'],\n [\"He doesn't want to go to school today.\",\n  'Él no quiere ir hoy a la escuela.'],\n [\"If you make new friends, don't forget the old ones.\",\n  'Si tú haces nuevos amigos, no olvides a los viejos.'],\n ['Try harder tomorrow.', 'Inténtalo mañana con más fuerza.'],\n ['Are you the owner of this house?', 'Eres el dueño de esta casa?'],\n ['Help us.', 'Ayúdanos.'],\n ['I stopped smoking.', 'Dejé de fumar.'],\n ['This song is known to everyone.', 'Esta canción es conocida por todos.'],\n ['You look hot.', 'Pareces acalorado.'],\n ['Tom put himself and his children at risk.',\n  'Tom se puso a si mismo y a sus hijos en peligro.'],\n ['I wish I were clever.', 'Ojalá fuera inteligente.'],\n ['I was in São Paulo in February.', 'Estaba en San Pablo en febrero.'],\n [\"I don't blame you for putting off our trip.\",\n  'No te recrimino que pospusieras nuestro viaje.'],\n ['Where do you think all the money goes?',\n  'Adónde te crees que se va todo el dinero?'],\n [\"I hope you're not mad at me.\", 'Espero que no estés molesto conmigo.'],\n [\"I haven't eaten there in a long time.\",\n  'No he comido ahí en mucho tiempo.'],\n ['Please take off your hat.', 'Por favor, sacate el sombrero.'],\n ['Tom twisted his ankle.', 'Tom se torció el tobillo.'],\n ['My uncle is very fond of fishing.', 'A mi abuelo le encanta pescar.'],\n ['Who is the tallest of the five?', 'Quién es el más alto de los cinco?'],\n [\"Tom and Mary didn't get along very well.\",\n  'Tom y Mary no se llevaban muy bien.'],\n ['I can resist everything but temptation.',\n  'Puedo resistirlo todo menos la tentación.'],\n ['Sanitary conditions in the refugee camps were terrible.',\n  'Las condiciones sanitarias en los campamentos de refugiados eran horribles.'],\n ['He was brave.', 'Él era valiente.'],\n ['Tom is deceitful.', 'Tom es falso.'],\n [\"He's already left.\", 'Él ya se fue.'],\n [\"Here's the key to your room.\", 'Aquí tienes la llave de tu habitación.'],\n ['I need a miracle.', 'Necesito un milagro.'],\n ['My glass is empty.', 'Mi vaso está vacío.'],\n [\"Tom and Mary don't have much time to talk together. Their children are always demanding their attention.\",\n  'Tom y Mary no tienen mucho tiempo para hablar juntos. Sus hijos siempre demandan su atención.'],\n ['It takes more time to prepare the car for getting painted than it takes for the actual paint job itself.',\n  'Tardas más en preparar el coche para la pintura que en el trabajo de pintura en sí.'],\n ['Do you need to leave today?', 'Es necesario que te marches hoy?'],\n [\"He's not home.\", 'No está en casa.'],\n [\"You shouldn't always follow the crowd.\",\n  'Uno no debería seguir siempre a la multitud.'],\n ['Nobody lives here.', 'Nadie vive aquí.'],\n ['Be creative.', 'Sed creativos.'],\n ['The boy tried to be a man and not cry.',\n  'El niño intentó ser un hombre y no llorar.'],\n ['Who does she think that she is?', 'Quién se cree que es?'],\n [\"Let's pool all our money.\", 'Juntemos todo el dinero.'],\n [\"I'm not sure there's a problem.\",\n  'No estoy seguro de que haya un problema.'],\n ['Tom already ate.', 'Tom ya comió.'],\n ['What an idiot!', 'Qué idiota!'],\n ['Are you embarrassed?', 'Tenés vergüenza?'],\n ['I only have a few books.', 'Solo tengo unos pocos libros.'],\n ['Where do they do that?', 'Dónde hacen eso?'],\n ['It makes no difference to me whether he likes baseball or football.',\n  'Me es indiferente si a él le gusta el béisbol o el fútbol.'],\n [\"Mom, where's the cat?\", 'Mamá, dónde está el gato?'],\n ['When did you start studying Latin?', 'Cuándo empezaste a estudiar latín?'],\n ['Tom has to look after Mary.', 'Tom tiene que cuidar a Mary.'],\n ['Tom stayed up all night studying.',\n  'Tom se quedó toda la noche estudiando.'],\n ['Show me another watch.', 'Enséñame otro reloj.'],\n ['Of course, many senior citizens are happy with retirement.',\n  'Por supuesto, muchos ciudadanos de la tercera edad son felices con el retiro.'],\n ['This dictionary, of which the third volume is missing, cost me a hundred dollars.',\n  'Este diccionario, del cual falta el tercer tomo, me costó cien dólares.'],\n ['We estimated the damage at 1000 dollars.',\n  'Hemos estimado el daño en mil dólares.'],\n ['Some did not know how to fight.', 'Algunos no sabían combatir.'],\n ['Health is essential to happiness.',\n  'La salud es indispensable para la felicidad.'],\n ['I could not get out of the stadium because of the crowd.',\n  'No pude salir del estadio debido al gentío.'],\n [\"Do you really think it's bad?\", 'De verdad piensas que es malo?'],\n ['How about going for a drive?', 'Qué te parece dar una vuelta en coche?'],\n [\"I didn't speak with Tom.\", 'No hablé con Tom.'],\n ['He arrived in time.', 'Él llegó a tiempo.'],\n ['Our sales are decreasing.', 'Nuestras ventas están cayendo.'],\n ['No way!', 'Minga!'],\n ['She came to see us yesterday.', 'Vino a vernos ayer.'],\n ['Tom must be angry with Mary.', 'Tom debe estar enojado con Mary.'],\n ['Tom is a good athlete.', 'Tom es un buen atleta.'],\n ['The old house was demolished.', 'La casa vieja fue demolida.'],\n ['How big you are!', 'Qué alto eres!'],\n ['We have yellow apples.', 'Tenemos manzanas amarillas.'],\n ['The only time you talk to me is when you need some money.',\n  'La única vez en que me hablas es cuando necesitas dinero.'],\n ['We slept under the stars.', 'Dormimos bajo las estrellas.'],\n ['He said that that girl had kissed him.',\n  'Él dijo que esa chica le había besado.'],\n ['There was a bus in the way.', 'Había un autobús en el camino.'],\n ['Coming back was a bad choice.', 'Regresar fue una mala elección.'],\n [\"How's everyone at the Hong Kong office?\",\n  'Cómo están todos en la oficina de Hong Kong?'],\n ['Tom sings quite well.', 'Tomás canta bastante bien.'],\n ['Absence makes the heart grow fonder.',\n  'Ni la ausencia ni el tiempo son nada cuando se ama.'],\n [\"Your soup's getting cold.\", 'Tu sopa se está enfriando.'],\n [\"You're driving me nuts.\", 'Me estás volviendo loco.'],\n [\"He's stronger than you.\", 'Él es más fuerte que tú.'],\n [\"I can't untie this knot.\", 'No puedo desatar este nudo.'],\n [\"I should've eaten more.\", 'Debería haber comido más.'],\n ['Is he a doctor?', 'Él es un médico?'],\n ['I read a letter.', 'Estoy leyendo una carta.'],\n ['You have no right to do this.', 'No tenéis derecho a hacer esto.'],\n [\"I can't make promises.\", 'No puedo prometer nada.'],\n [\"It's been thirty years since we got married.\",\n  'Han pasado treinta años desde que nos casamos.'],\n ['The violence lasted for two weeks.',\n  'La violencia perduró por dos semanas.'],\n ['Women generally live longer than men.',\n  'Las mujeres generalmente viven más que los hombres.'],\n ['I thought Tom would want to try some Japanese food.',\n  'Pensé que a Tom le gustaría probar un poco de comida japonesa.'],\n [\"I'm sure there is nobody as kind as you are the whole world.\",\n  'Yo estoy seguro de que no hay nadie tan gentil como tú en todo el mundo.'],\n [\"Don't laugh at him.\", 'No os riais de él.'],\n ['You must remove your shoes before entering a house.',\n  'Tú debes quitarte los zapatos antes de entrar a una casa.'],\n ['Can we move on?', 'Podemos continuar?'],\n ['She was soaked from head to foot.', 'Ella se empapó de pies a cabeza.'],\n [\"I didn't look under the couch.\", 'No miré bajo el sofá.'],\n ['When I was a boy, I thought that I wanted to be a doctor.',\n  'Cuando era niño, pensaba que quería ser médico.'],\n [\"I'll be with you in a second.\", 'Estaré contigo en un segundo.'],\n ['I agree with you that we need more women in this company.',\n  'Estoy de acuerdo con usted en que necesitamos más mujeres en esta empresa.'],\n ['How could that happen?', 'Cómo pudo ocurrir eso?'],\n [\"What's Germany's largest lake?\", 'Cuál es el lago más grande de Alemania?'],\n ['My mother has sold everything that is dear to her.',\n  'Mi madre vendió todo lo que era valioso para ella.'],\n ['Is that it?', 'Es ese?'],\n ['What are they after?', 'Qué persiguen ellos?'],\n ['I went swimming in the sea.', 'Fui a nadar al mar.'],\n ['Mehmed Talat was assassinated in Berlin in 1921.',\n  'Mehmed Talat fue asesinado en Berlín en el año 1921.'],\n [\"I ran all the way here and I'm out of breath.\",\n  'Vine corriendo todo el camino hasta aquí y estoy sin aliento.'],\n ['I can often hear pigeons cooing outside my bedroom window.',\n  'A menudo oigo a las palomas arrullar desde la ventana de mi habitación'],\n [\"It didn't work.\", 'No funcionó.'],\n [\"If I weren't sick, I'd join you.\",\n  'Si no estuviera enfermo, me uniría a vosotros.'],\n ['The work begins to get more difficult.',\n  'El trabajo empieza a ser más difícil.'],\n ['What does your father do?', 'Qué hace tu padre?'],\n ['I caught up with the others.', 'Alcancé a los demás.'],\n ['I was lonely.', 'Estaba solo.'],\n ['Tom is very nice to me.', 'Tom me cae bien.'],\n ['What makes you think that Tom is planning to ask Mary to marry him?',\n  'Qué te hace pensar que Tom está planeando pedirle a Mary que se case con él?'],\n ['He is always laughing.', 'Siempre se está riendo.'],\n ['The station is dead ahead.', 'La estación está justo enfrente.'],\n [\"Tom apologized for not doing what he had promised he'd do.\",\n  'Tom se disculpó por no cumplir sus promesas.'],\n ['Tom arrived by car.', 'Tom vino en auto.'],\n [\"I'm kind of happy for you.\", 'Estoy algo feliz por ti.'],\n ['How much did you pay?', 'Cuánto has pagado?'],\n ['The door is closing.', 'La puerta se está cerrando.'],\n [\"I know what I'm saying.\", 'Yo sé lo que estoy diciendo.'],\n ['My mother makes the best cakes in the world.',\n  'Mi madre hace las mejores tartas del mundo.'],\n ['There is only one thing to do.', 'Sólo hay una cosa por hacer.'],\n [\"She's alive! She was drowning, but her father saved her.\",\n  'Está viva! Se estaba ahogando, pero su padre la salvó.'],\n ['Take over.', 'Asume el mando.'],\n ['We agree.', 'Estamos de acuerdo.'],\n ['What did Tom give you for Christmas?', 'Qué te regaló Tom para Navidad?'],\n ['I despise Tom.', 'Yo menosprecio a Tom.'],\n [\"Because some urgent business came up, he wasn't able to go to the concert.\",\n  'Debido a que surgió un asunto urgente, él no pudo ir al concierto.'],\n [\"Who's taking responsibility for this problem?\",\n  'Quién asumirá responsabilidad por este problema?'],\n ['You shall not marry my daughter!', 'No te vas a casar con mi hija!'],\n ['Honk the horn.', 'Toca el claxon.'],\n ['Your message has been received.', 'Han recibido tu mensaje.'],\n ...]"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How boring! => Qué aburrimiento!\n",
      "I love sports. => Adoro el deporte.\n",
      "Would you like to swap jobs? => Te gustaría que intercambiemos los trabajos?\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(sentences_en[i], \"=>\", sentences_es[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "vocab_size = 20\n",
    "max_length = 50\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length)\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_es.adapt([f\"startofseq {s} endofseq\" for s in sentences_es])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(sentences_en[i], \"=>\", sentences_es[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "['',\n '[UNK]',\n 'the',\n 'i',\n 'to',\n 'you',\n 'tom',\n 'a',\n 'is',\n 'he',\n 'in',\n 'of',\n 'that',\n 'it',\n 'was',\n 'do',\n 'have',\n 'this',\n 'me',\n 'my',\n 'for',\n 'she',\n 'dont',\n 'are',\n 'what',\n 'his',\n 'mary',\n 'we',\n 'your',\n 'on',\n 'be',\n 'with',\n 'want',\n 'not',\n 'im',\n 'and',\n 'like',\n 'at',\n 'know',\n 'him']"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_en.get_vocabulary()[:3000]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "['',\n '[UNK]',\n 'startofseq',\n 'endofseq',\n 'de',\n 'que',\n 'a',\n 'no',\n 'tom',\n 'la',\n 'el',\n 'en',\n 'es',\n 'un',\n 'me',\n 'se',\n 'por',\n 'lo',\n 'una',\n 'su']"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_es.get_vocabulary()[:30]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "X_train = tf.constant(sentences_en[:100_000])\n",
    "X_valid = tf.constant(sentences_en[100_000:])\n",
    "X_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
    "X_valid_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[100_000:]])\n",
    "Y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
    "Y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                                                    mask_zero=True)\n",
    "\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fbcca1a12b0>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/14/93/337258e3dd2234e9d438603779013f9eead3a021b3fa0ba1e9b5bd30d57b/tfds_nightly-4.6.0.dev202209090045-py3-none-any.whl\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "# !pip3 install -q tfds-nightly tensorflow matplotlib"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "<KerasTensor: shape=(None, 50) dtype=int64 (created by layer 'text_vectorization_9')>"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_ids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "encoder = tf.keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
    "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "Y_proba = output_layer(decoder_outputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.7049WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f83734178b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f83734178b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f83734178b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "3125/3125 [==============================] - 1542s 490ms/step - loss: 0.1483 - accuracy: 0.7049 - val_loss: 0.1315 - val_accuracy: 0.7226\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7f835f76cf70>"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=1, validation_data=((X_valid, X_valid_dec), Y_valid))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f835f59faf0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('model',), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f835f59faf0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('model',), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x7f835f59faf0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('model',), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_save_restore_functions.<locals>.save_fn at 0x7f8351ba5dc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('saveable_factory', 'saveables'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_save_restore_functions.<locals>.save_fn at 0x7f8351ba5dc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('saveable_factory', 'saveables'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function trace_save_restore_functions.<locals>.save_fn at 0x7f8351ba5dc0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('saveable_factory', 'saveables'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function trace_save_restore_functions.<locals>.restore_fn at 0x7f835f9a40d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('saveables', 'tensor_structure'), but source function had ('saveables',)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_save_restore_functions.<locals>.restore_fn at 0x7f835f9a40d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('saveables', 'tensor_structure'), but source function had ('saveables',)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function trace_save_restore_functions.<locals>.restore_fn at 0x7f835f9a40d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('saveables', 'tensor_structure'), but source function had ('saveables',)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function trace_save_restore_functions.<locals>.save_fn at 0x7f834f840670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('saveable_factory', 'saveables'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_save_restore_functions.<locals>.save_fn at 0x7f834f840670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('saveable_factory', 'saveables'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function trace_save_restore_functions.<locals>.save_fn at 0x7f834f840670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('saveable_factory', 'saveables'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function trace_save_restore_functions.<locals>.restore_fn at 0x7f8351bd3e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('saveables', 'tensor_structure'), but source function had ('saveables',)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_save_restore_functions.<locals>.restore_fn at 0x7f8351bd3e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('saveables', 'tensor_structure'), but source function had ('saveables',)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function trace_save_restore_functions.<locals>.restore_fn at 0x7f8351bd3e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('saveables', 'tensor_structure'), but source function had ('saveables',)\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8351bd3dc0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8351bd3dc0>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8351bd3dc0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8351bd3dc0>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8351bd3dc0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8351bd3dc0>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70> and will run it as-is.\n",
      "Cause: could not parse the source code of <function _trace_resource_initializers.<locals>._wrap_obj_initializer.<locals>.<lambda> at 0x7f8360016f70>: no matching AST found among candidates:\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: /Users/nurrizkyimani/PycharmProjects/nlp-clickbait-transfers/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/nurrizkyimani/PycharmProjects/nlp-clickbait-transfers/assets\n"
     ]
    }
   ],
   "source": [
    "# !pwd\n",
    "model.save('/Users/nurrizkyimani/PycharmProjects/nlp-clickbait-transfers')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([1.88530139e-05, 5.05020082e-01, 2.03813543e-05, 1.82372372e-04,\n       1.02445541e-03, 2.18955814e-04, 1.48296612e-03, 1.05833985e-01,\n       1.88816793e-03, 4.29831911e-03, 1.02007168e-03, 2.65815004e-04,\n       3.59552703e-03, 2.21436465e-04, 3.56876820e-01, 3.70510714e-03,\n       7.22576573e-04, 1.32989828e-02, 1.46520339e-04, 1.58648807e-04],\n      dtype=float32)"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_en = \"I like soccer\"\n",
    "translation = \"\"\n",
    "\n",
    "word_idx = 1\n",
    "  # print(\"print word_idx\", word_idx)\n",
    "X = np.array([sentence_en])  # encoder input\n",
    "# print(X)\n",
    "\n",
    "X_dec = np.array([\"startofseq \" + translation])  # decoder input\n",
    "# print(X_dec)\n",
    "\n",
    "\n",
    "# model.predict()\n",
    "y_proba = model.predict((X, X_dec))[0,10]# last token's probas\n",
    "# print(y_proba)\n",
    "\n",
    "y_proba\n",
    "\n",
    "predicted_word_id = np.argmax(y_proba)\n",
    "# print(predicted_word_id)\n",
    "\n",
    "# test = np.argmax(y_proba)\n",
    "\n",
    "# print(test)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 90ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[1.88530139e-05 5.05020082e-01 2.03813543e-05 1.82372372e-04\n",
      " 1.02445541e-03 2.18955814e-04 1.48296612e-03 1.05833985e-01\n",
      " 1.88816793e-03 4.29831911e-03 1.02007168e-03 2.65815004e-04\n",
      " 3.59552703e-03 2.21436465e-04 3.56876820e-01 3.70510714e-03\n",
      " 7.22576573e-04 1.32989828e-02 1.46520339e-04 1.58648807e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "[1.88530194e-05 5.05020201e-01 2.03814161e-05 1.82372678e-04\n",
      " 1.02445704e-03 2.18956295e-04 1.48296787e-03 1.05834186e-01\n",
      " 1.88817026e-03 4.29832656e-03 1.02007296e-03 2.65815062e-04\n",
      " 3.59552936e-03 2.21437032e-04 3.56876373e-01 3.70511157e-03\n",
      " 7.22577446e-04 1.32989986e-02 1.46520513e-04 1.58648996e-04]\n",
      "1\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "[1.88530194e-05 5.05020201e-01 2.03814161e-05 1.82372678e-04\n",
      " 1.02445704e-03 2.18956295e-04 1.48296787e-03 1.05834186e-01\n",
      " 1.88817026e-03 4.29832656e-03 1.02007296e-03 2.65815062e-04\n",
      " 3.59552936e-03 2.21437032e-04 3.56876373e-01 3.70511157e-03\n",
      " 7.22577446e-04 1.32989986e-02 1.46520513e-04 1.58648996e-04]\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": "''"
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate(sentence_en):\n",
    "    translation = \"\"\n",
    "\n",
    "    # print(max_length)\n",
    "    for word_idx in range(max_length):\n",
    "\n",
    "        # print(\"print word_idx\", word_idx)\n",
    "        X = np.array([sentence_en])  # encoder input\n",
    "        # print(X)\n",
    "\n",
    "        X_dec = np.array([\"startofseq \" + translation])  # decoder input\n",
    "        # print(X_dec)\n",
    "\n",
    "        y_proba = model.predict((X, X_dec))[0, word_idx]  # last token's probas\n",
    "        # print(y_proba)\n",
    "\n",
    "        predicted_word_id = np.argmax(y_proba)\n",
    "        print(predicted_word_id)\n",
    "\n",
    "        # predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
    "        # print(predicted_word)\n",
    "\n",
    "        # if predicted_word == \"endofseq\":\n",
    "        #     break\n",
    "        # translation += \" \" + predicted_word\n",
    "\n",
    "    return translation.strip()\n",
    "\n",
    "\n",
    "translate(\"I like soccer\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['startofseq ']\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "['startofseq  [UNK]']\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "['startofseq  [UNK] [UNK]']\n",
      "1/1 [==============================] - 0s 60ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "'[UNK] [UNK]'"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Style Transfer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
    "    name=\"imdb_reviews\",\n",
    "    split=[\"train[:90%]\", \"train[90%:]\", \"test\"],\n",
    "    as_supervised=True\n",
    ")\n",
    "tf.random.set_seed(42)\n",
    "train_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\n",
    "valid_set = raw_valid_set.batch(32).prefetch(1)\n",
    "test_set = raw_test_set.batch(32).prefetch(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n",
      "Label: 0\n",
      "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n",
      "Label: 0\n",
      "Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.\n",
      "Label: 0\n",
      "This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.\n",
      "Label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-09 20:15:42.288545: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for review, label in raw_train_set.take(4):\n",
    "    print(review.numpy().decode(\"utf-8\"))\n",
    "    print(\"Label:\", label.numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
    "text_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "<keras.layers.preprocessing.text_vectorization.TextVectorization at 0x7f839c871550>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_vec_layer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Embedding(vocab_size, embed_size),\n",
    "    tf.keras.layers.GRU(128),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "token_ids = text_vec_layer(inputs)\n",
    "mask = tf.math.not_equal(token_ids, 0)\n",
    "Z = tf.keras.layers.Embedding(vocab_size, embed_size)(token_ids)\n",
    "Z = tf.keras.layers.GRU(128, dropout=0.2)(Z, mask=mask)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(Z)\n",
    "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 5), dtype=int64, numpy=\narray([[ 86,  18,   0,   0,   0],\n       [ 11,   7,   1, 116, 217]])>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer([\"Great movie!\", \"This is DiCaprio's best role.\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [84]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow_hub\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mhub\u001B[39;00m\n\u001B[1;32m      4\u001B[0m os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTFHUB_CACHE_DIR\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmy_tfhub_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      5\u001B[0m model \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mSequential([\n\u001B[0;32m----> 6\u001B[0m     \u001B[43mhub\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mKerasLayer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhttps://tfhub.dev/google/universal-sentence-encoder/4\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mtrainable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstring\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m      8\u001B[0m     tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mDense(\u001B[38;5;241m2\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      9\u001B[0m     tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mDense(\u001B[38;5;241m1\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msigmoid\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     10\u001B[0m ])\n\u001B[1;32m     11\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary_crossentropy\u001B[39m\u001B[38;5;124m\"\u001B[39m, optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnadam\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     12\u001B[0m               metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     13\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(train_set, validation_data\u001B[38;5;241m=\u001B[39mvalid_set, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py:153\u001B[0m, in \u001B[0;36mKerasLayer.__init__\u001B[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001B[0m\n\u001B[1;32m    149\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_shape \u001B[38;5;241m=\u001B[39m data_structures\u001B[38;5;241m.\u001B[39mNoDependency(\n\u001B[1;32m    150\u001B[0m       _convert_nest_to_shapes(output_shape))\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load_options \u001B[38;5;241m=\u001B[39m load_options\n\u001B[0;32m--> 153\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_func \u001B[38;5;241m=\u001B[39m \u001B[43mload_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_options\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_training_argument \u001B[38;5;241m=\u001B[39m func_has_training_argument(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_func)\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_hub_module_v1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_func, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_is_hub_module_v1\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py:449\u001B[0m, in \u001B[0;36mload_module\u001B[0;34m(handle, tags, load_options)\u001B[0m\n\u001B[1;32m    447\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:  \u001B[38;5;66;03m# Expected before TF2.4.\u001B[39;00m\n\u001B[1;32m    448\u001B[0m     set_load_options \u001B[38;5;241m=\u001B[39m load_options\n\u001B[0;32m--> 449\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodule_v2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mset_load_options\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow_hub/module_v2.py:92\u001B[0m, in \u001B[0;36mload\u001B[0;34m(handle, tags, options)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m     91\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected a string, got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m handle)\n\u001B[0;32m---> 92\u001B[0m module_path \u001B[38;5;241m=\u001B[39m \u001B[43mresolve\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     93\u001B[0m is_hub_module_v1 \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mio\u001B[38;5;241m.\u001B[39mgfile\u001B[38;5;241m.\u001B[39mexists(\n\u001B[1;32m     94\u001B[0m     native_module\u001B[38;5;241m.\u001B[39mget_module_proto_path(module_path))\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tags \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m is_hub_module_v1:\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow_hub/module_v2.py:47\u001B[0m, in \u001B[0;36mresolve\u001B[0;34m(handle)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mresolve\u001B[39m(handle):\n\u001B[1;32m     24\u001B[0m   \u001B[38;5;124;03m\"\"\"Resolves a module handle into a path.\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \n\u001B[1;32m     26\u001B[0m \u001B[38;5;124;03m  This function works both for plain TF2 SavedModels and the legacy TF1 Hub\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;124;03m    A string representing the Module path.\u001B[39;00m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m---> 47\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mregistry\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresolver\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow_hub/registry.py:51\u001B[0m, in \u001B[0;36mMultiImplRegister.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m impl \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mreversed\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_impls):\n\u001B[1;32m     50\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m impl\u001B[38;5;241m.\u001B[39mis_supported(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimpl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     52\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     fails\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mtype\u001B[39m(impl)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow_hub/compressed_module_resolver.py:67\u001B[0m, in \u001B[0;36mHttpCompressedFileResolver.__call__\u001B[0;34m(self, handle)\u001B[0m\n\u001B[1;32m     63\u001B[0m   response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_urlopen(request)\n\u001B[1;32m     64\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m resolver\u001B[38;5;241m.\u001B[39mDownloadManager(handle)\u001B[38;5;241m.\u001B[39mdownload_and_uncompress(\n\u001B[1;32m     65\u001B[0m       response, tmp_dir)\n\u001B[0;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mresolver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43matomic_download\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[43m                                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_lock_file_timeout_sec\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow_hub/resolver.py:418\u001B[0m, in \u001B[0;36matomic_download\u001B[0;34m(handle, download_fn, module_dir, lock_file_timeout_sec)\u001B[0m\n\u001B[1;32m    416\u001B[0m logging\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDownloading TF-Hub Module \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, handle)\n\u001B[1;32m    417\u001B[0m tf\u001B[38;5;241m.\u001B[39mcompat\u001B[38;5;241m.\u001B[39mv1\u001B[38;5;241m.\u001B[39mgfile\u001B[38;5;241m.\u001B[39mMakeDirs(tmp_dir)\n\u001B[0;32m--> 418\u001B[0m \u001B[43mdownload_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtmp_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    419\u001B[0m \u001B[38;5;66;03m# Write module descriptor to capture information about which module was\u001B[39;00m\n\u001B[1;32m    420\u001B[0m \u001B[38;5;66;03m# downloaded by whom and when. The file stored at the same level as a\u001B[39;00m\n\u001B[1;32m    421\u001B[0m \u001B[38;5;66;03m# directory in order to keep the content of the 'model_dir' exactly as it\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    426\u001B[0m \u001B[38;5;66;03m# module caching protocol and no code in the TF-Hub library reads its\u001B[39;00m\n\u001B[1;32m    427\u001B[0m \u001B[38;5;66;03m# content.\u001B[39;00m\n\u001B[1;32m    428\u001B[0m _write_module_descriptor_file(handle, module_dir)\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow_hub/compressed_module_resolver.py:64\u001B[0m, in \u001B[0;36mHttpCompressedFileResolver.__call__.<locals>.download\u001B[0;34m(handle, tmp_dir)\u001B[0m\n\u001B[1;32m     61\u001B[0m request \u001B[38;5;241m=\u001B[39m urllib\u001B[38;5;241m.\u001B[39mrequest\u001B[38;5;241m.\u001B[39mRequest(\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_append_compressed_format_query(handle))\n\u001B[1;32m     63\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_urlopen(request)\n\u001B[0;32m---> 64\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mresolver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDownloadManager\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_and_uncompress\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtmp_dir\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow_hub/resolver.py:189\u001B[0m, in \u001B[0;36mDownloadManager.download_and_uncompress\u001B[0;34m(self, fileobj, dst_path)\u001B[0m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;124;03m\"\"\"Streams the content for the 'fileobj' and stores the result in dst_path.\u001B[39;00m\n\u001B[1;32m    180\u001B[0m \n\u001B[1;32m    181\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;124;03m  ValueError: Unknown object encountered inside the TAR file.\u001B[39;00m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 189\u001B[0m   \u001B[43mfile_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_tarfile_to_destination\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    190\u001B[0m \u001B[43m      \u001B[49m\u001B[43mfileobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdst_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_function\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_log_progress\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    191\u001B[0m   total_size_str \u001B[38;5;241m=\u001B[39m tf_utils\u001B[38;5;241m.\u001B[39mbytes_to_readable_str(\n\u001B[1;32m    192\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_total_bytes_downloaded, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    193\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_print_download_progress_msg(\n\u001B[1;32m    194\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDownloaded \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, Total size: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_url, total_size_str),\n\u001B[1;32m    195\u001B[0m       flush\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow_hub/file_utils.py:52\u001B[0m, in \u001B[0;36mextract_tarfile_to_destination\u001B[0;34m(fileobj, dst_path, log_function)\u001B[0m\n\u001B[1;32m     49\u001B[0m abs_target_path \u001B[38;5;241m=\u001B[39m merge_relative_path(dst_path, tarinfo\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tarinfo\u001B[38;5;241m.\u001B[39misfile():\n\u001B[0;32m---> 52\u001B[0m   \u001B[43mextract_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtgz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarinfo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mabs_target_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_function\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_function\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m tarinfo\u001B[38;5;241m.\u001B[39misdir():\n\u001B[1;32m     54\u001B[0m   tf\u001B[38;5;241m.\u001B[39mcompat\u001B[38;5;241m.\u001B[39mv1\u001B[38;5;241m.\u001B[39mgfile\u001B[38;5;241m.\u001B[39mMakeDirs(abs_target_path)\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/site-packages/tensorflow_hub/file_utils.py:35\u001B[0m, in \u001B[0;36mextract_file\u001B[0;34m(tgz, tarinfo, dst_path, buffer_size, log_function)\u001B[0m\n\u001B[1;32m     33\u001B[0m dst \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mcompat\u001B[38;5;241m.\u001B[39mv1\u001B[38;5;241m.\u001B[39mgfile\u001B[38;5;241m.\u001B[39mGFile(dst_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m---> 35\u001B[0m   buf \u001B[38;5;241m=\u001B[39m \u001B[43msrc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m buf:\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/tarfile.py:695\u001B[0m, in \u001B[0;36m_FileInFile.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    694\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreadinto\u001B[39m(\u001B[38;5;28mself\u001B[39m, b):\n\u001B[0;32m--> 695\u001B[0m     buf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    696\u001B[0m     b[:\u001B[38;5;28mlen\u001B[39m(buf)] \u001B[38;5;241m=\u001B[39m buf\n\u001B[1;32m    697\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(buf)\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/tarfile.py:684\u001B[0m, in \u001B[0;36m_FileInFile.read\u001B[0;34m(self, size)\u001B[0m\n\u001B[1;32m    682\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m data:\n\u001B[1;32m    683\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfileobj\u001B[38;5;241m.\u001B[39mseek(offset \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition \u001B[38;5;241m-\u001B[39m start))\n\u001B[0;32m--> 684\u001B[0m     b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfileobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlength\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    685\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(b) \u001B[38;5;241m!=\u001B[39m length:\n\u001B[1;32m    686\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ReadError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munexpected end of data\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/tarfile.py:521\u001B[0m, in \u001B[0;36m_Stream.read\u001B[0;34m(self, size)\u001B[0m\n\u001B[1;32m    519\u001B[0m \u001B[38;5;124;03m\"\"\"Return the next size number of bytes from the stream.\"\"\"\u001B[39;00m\n\u001B[1;32m    520\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 521\u001B[0m buf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(buf)\n\u001B[1;32m    523\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m buf\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/tarfile.py:539\u001B[0m, in \u001B[0;36m_Stream._read\u001B[0;34m(self, size)\u001B[0m\n\u001B[1;32m    537\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    538\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 539\u001B[0m     buf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfileobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbufsize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    540\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m buf:\n\u001B[1;32m    541\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/http/client.py:459\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    457\u001B[0m     \u001B[38;5;66;03m# Amount is given, implement using readinto\u001B[39;00m\n\u001B[1;32m    458\u001B[0m     b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mbytearray\u001B[39m(amt)\n\u001B[0;32m--> 459\u001B[0m     n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadinto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmemoryview\u001B[39m(b)[:n]\u001B[38;5;241m.\u001B[39mtobytes()\n\u001B[1;32m    461\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    462\u001B[0m     \u001B[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001B[39;00m\n\u001B[1;32m    463\u001B[0m     \u001B[38;5;66;03m# and self.chunked\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/http/client.py:503\u001B[0m, in \u001B[0;36mHTTPResponse.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    498\u001B[0m         b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmemoryview\u001B[39m(b)[\u001B[38;5;241m0\u001B[39m:\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength]\n\u001B[1;32m    500\u001B[0m \u001B[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001B[39;00m\n\u001B[1;32m    501\u001B[0m \u001B[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001B[39;00m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;66;03m# (for example, reading in 1k chunks)\u001B[39;00m\n\u001B[0;32m--> 503\u001B[0m n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadinto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m n \u001B[38;5;129;01mand\u001B[39;00m b:\n\u001B[1;32m    505\u001B[0m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[1;32m    506\u001B[0m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[1;32m    507\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_conn()\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/socket.py:669\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    667\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    668\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 669\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    670\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    671\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/ssl.py:1241\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[0;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[1;32m   1237\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1238\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1239\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[1;32m   1240\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[0;32m-> 1241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1242\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1243\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[0;32m~/.conda/envs/workspace/lib/python3.8/ssl.py:1099\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[0;34m(self, len, buffer)\u001B[0m\n\u001B[1;32m   1097\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1098\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1099\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1100\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1101\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"my_tfhub_cache\"\n",
    "model = tf.keras.Sequential([\n",
    "    hub.KerasLayer(\n",
    "        \"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "        trainable=True,\n",
    "        dtype=tf.string,\n",
    "        input_shape=[]),\n",
    "    tf.keras.layers.Dense(2, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_set, validation_data=valid_set, epochs=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow>=2.0.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (2.9.2)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (1.2.0)\r\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (2.9.1)\r\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (2.9.0)\r\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (3.19.4)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (1.14.1)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (0.2.0)\r\n",
      "Requirement already satisfied: setuptools in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (63.4.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (4.3.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (1.16.0)\r\n",
      "Requirement already satisfied: packaging in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (21.3)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (3.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (1.23.2)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (3.7.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (1.48.1)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (0.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (1.6.3)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (1.1.2)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (14.0.6)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (1.1.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (0.26.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (2.9.0)\r\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow>=2.0.0) (1.12)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow>=2.0.0) (0.37.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (0.4.6)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (3.4.1)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (2.28.1)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (1.8.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (2.2.2)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (2.11.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (0.6.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from packaging->tensorflow>=2.0.0) (3.0.9)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (5.2.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (4.9)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (0.2.8)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (4.12.0)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (2.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (2022.6.15)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (1.26.11)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (2.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (3.8.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.0.0) (3.2.0)\r\n",
      "Requirement already satisfied: tensorflow-hub in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (0.12.0)\r\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow-hub) (1.23.2)\r\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (from tensorflow-hub) (3.19.4)\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install \"tensorflow>=2.0.0\"\n",
    "# !pip install --upgrade tensorflow-hub"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/nurrizkyimani/.conda/envs/workspace/lib/python3.8/site-packages (2.9.0)\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install keras"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\r\n",
      "  Downloading tensorflow-2.10.0-cp38-cp38-macosx_10_14_x86_64.whl (241.2 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m241.2/241.2 MB\u001B[0m \u001B[31m1.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:03\u001B[0m\r\n",
      "\u001B[?25hCollecting absl-py>=1.0.0\r\n",
      "  Using cached absl_py-1.2.0-py3-none-any.whl (123 kB)\r\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\r\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.27.0-cp38-cp38-macosx_10_14_x86_64.whl (1.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting keras<2.11,>=2.10.0\r\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting packaging\r\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.8/40.8 kB\u001B[0m \u001B[31m1.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting libclang>=13.0.0\r\n",
      "  Using cached libclang-14.0.6-py2.py3-none-macosx_10_9_x86_64.whl (13.2 MB)\r\n",
      "Collecting termcolor>=1.1.0\r\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\r\n",
      "Collecting gast<=0.4.0,>=0.2.1\r\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\r\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\r\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m438.7/438.7 kB\u001B[0m \u001B[31m2.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting grpcio<2.0,>=1.24.3\r\n",
      "  Using cached grpcio-1.48.1-cp38-cp38-macosx_10_10_x86_64.whl (4.5 MB)\r\n",
      "Collecting tensorboard<2.11,>=2.10\r\n",
      "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.9/5.9 MB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting keras-preprocessing>=1.1.1\r\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\r\n",
      "Collecting google-pasta>=0.1.1\r\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\r\n",
      "Collecting numpy>=1.20\r\n",
      "  Using cached numpy-1.23.2-cp38-cp38-macosx_10_9_x86_64.whl (18.1 MB)\r\n",
      "Collecting h5py>=2.9.0\r\n",
      "  Using cached h5py-3.7.0-cp38-cp38-macosx_10_9_x86_64.whl (3.2 MB)\r\n",
      "Collecting astunparse>=1.6.0\r\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\r\n",
      "Collecting typing-extensions>=3.6.6\r\n",
      "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\r\n",
      "Collecting wrapt>=1.11.0\r\n",
      "  Using cached wrapt-1.14.1-cp38-cp38-macosx_10_9_x86_64.whl (35 kB)\r\n",
      "Collecting opt-einsum>=2.3.2\r\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\r\n",
      "Collecting protobuf<3.20,>=3.9.2\r\n",
      "  Using cached protobuf-3.19.4-cp38-cp38-macosx_10_9_x86_64.whl (961 kB)\r\n",
      "Collecting setuptools\r\n",
      "  Downloading setuptools-65.3.0-py3-none-any.whl (1.2 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting flatbuffers>=2.0\r\n",
      "  Using cached flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\r\n",
      "Collecting six>=1.12.0\r\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\r\n",
      "Collecting wheel<1.0,>=0.23.0\r\n",
      "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\r\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\r\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\r\n",
      "Collecting requests<3,>=2.21.0\r\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.8/62.8 kB\u001B[0m \u001B[31m536.9 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting tensorboard-plugin-wit>=1.6.0\r\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\r\n",
      "Collecting markdown>=2.6.8\r\n",
      "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\r\n",
      "Collecting werkzeug>=1.0.1\r\n",
      "  Using cached Werkzeug-2.2.2-py3-none-any.whl (232 kB)\r\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\r\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\r\n",
      "Collecting google-auth<3,>=1.6.3\r\n",
      "  Using cached google_auth-2.11.0-py2.py3-none-any.whl (167 kB)\r\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\r\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m98.3/98.3 kB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting rsa<5,>=3.1.4\r\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\r\n",
      "Collecting pyasn1-modules>=0.2.1\r\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\r\n",
      "Collecting cachetools<6.0,>=2.0.0\r\n",
      "  Using cached cachetools-5.2.0-py3-none-any.whl (9.3 kB)\r\n",
      "Collecting requests-oauthlib>=0.7.0\r\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\r\n",
      "Collecting importlib-metadata>=4.4\r\n",
      "  Using cached importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\r\n",
      "Collecting urllib3<1.27,>=1.21.1\r\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m140.4/140.4 kB\u001B[0m \u001B[31m1.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting charset-normalizer<3,>=2\r\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\r\n",
      "Collecting idna<4,>=2.5\r\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.2/61.2 kB\u001B[0m \u001B[31m1.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting certifi>=2017.4.17\r\n",
      "  Downloading certifi-2022.6.15.1-py3-none-any.whl (160 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m160.4/160.4 kB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting MarkupSafe>=2.1.1\r\n",
      "  Downloading MarkupSafe-2.1.1-cp38-cp38-macosx_10_9_x86_64.whl (13 kB)\r\n",
      "Collecting zipp>=0.5\r\n",
      "  Downloading zipp-3.8.1-py3-none-any.whl (5.6 kB)\r\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\r\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\r\n",
      "Collecting oauthlib>=3.0.0\r\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\r\n",
      "Installing collected packages: termcolor, tensorboard-plugin-wit, pyasn1, libclang, keras, flatbuffers, zipp, wrapt, wheel, urllib3, typing-extensions, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, setuptools, rsa, pyparsing, pyasn1-modules, protobuf, oauthlib, numpy, MarkupSafe, idna, gast, charset-normalizer, certifi, cachetools, absl-py, werkzeug, requests, packaging, opt-einsum, keras-preprocessing, importlib-metadata, h5py, grpcio, google-pasta, google-auth, astunparse, requests-oauthlib, markdown, google-auth-oauthlib, tensorboard, tensorflow\r\n",
      "  Attempting uninstall: termcolor\r\n",
      "    Found existing installation: termcolor 1.1.0\r\n",
      "    Uninstalling termcolor-1.1.0:\r\n",
      "      Successfully uninstalled termcolor-1.1.0\r\n",
      "  Attempting uninstall: tensorboard-plugin-wit\r\n",
      "    Found existing installation: tensorboard-plugin-wit 1.8.1\r\n",
      "    Uninstalling tensorboard-plugin-wit-1.8.1:\r\n",
      "      Successfully uninstalled tensorboard-plugin-wit-1.8.1\r\n",
      "  Attempting uninstall: pyasn1\r\n",
      "    Found existing installation: pyasn1 0.4.8\r\n",
      "    Uninstalling pyasn1-0.4.8:\r\n",
      "      Successfully uninstalled pyasn1-0.4.8\r\n",
      "  Attempting uninstall: libclang\r\n",
      "    Found existing installation: libclang 14.0.6\r\n",
      "    Uninstalling libclang-14.0.6:\r\n",
      "      Successfully uninstalled libclang-14.0.6\r\n",
      "  Attempting uninstall: keras\r\n",
      "    Found existing installation: keras 2.9.0\r\n",
      "    Uninstalling keras-2.9.0:\r\n",
      "      Successfully uninstalled keras-2.9.0\r\n",
      "  Attempting uninstall: flatbuffers\r\n",
      "    Found existing installation: flatbuffers 1.12\r\n",
      "    Uninstalling flatbuffers-1.12:\r\n",
      "      Successfully uninstalled flatbuffers-1.12\r\n",
      "  Attempting uninstall: zipp\r\n",
      "    Found existing installation: zipp 3.8.0\r\n",
      "    Uninstalling zipp-3.8.0:\r\n",
      "      Successfully uninstalled zipp-3.8.0\r\n",
      "  Attempting uninstall: wrapt\r\n",
      "    Found existing installation: wrapt 1.14.1\r\n",
      "    Uninstalling wrapt-1.14.1:\r\n",
      "      Successfully uninstalled wrapt-1.14.1\r\n",
      "  Attempting uninstall: wheel\r\n",
      "    Found existing installation: wheel 0.37.1\r\n",
      "    Uninstalling wheel-0.37.1:\r\n",
      "      Successfully uninstalled wheel-0.37.1\r\n",
      "  Attempting uninstall: urllib3\r\n",
      "    Found existing installation: urllib3 1.26.11\r\n",
      "    Uninstalling urllib3-1.26.11:\r\n",
      "      Successfully uninstalled urllib3-1.26.11\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.3.0\r\n",
      "    Uninstalling typing_extensions-4.3.0:\r\n",
      "      Successfully uninstalled typing_extensions-4.3.0\r\n",
      "  Attempting uninstall: tensorflow-io-gcs-filesystem\r\n",
      "    Found existing installation: tensorflow-io-gcs-filesystem 0.26.0\r\n",
      "    Uninstalling tensorflow-io-gcs-filesystem-0.26.0:\r\n",
      "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.26.0\r\n",
      "  Attempting uninstall: tensorflow-estimator\r\n",
      "    Found existing installation: tensorflow-estimator 2.9.0\r\n",
      "    Uninstalling tensorflow-estimator-2.9.0:\r\n",
      "      Successfully uninstalled tensorflow-estimator-2.9.0\r\n",
      "  Attempting uninstall: tensorboard-data-server\r\n",
      "    Found existing installation: tensorboard-data-server 0.6.1\r\n",
      "    Uninstalling tensorboard-data-server-0.6.1:\r\n",
      "      Successfully uninstalled tensorboard-data-server-0.6.1\r\n",
      "  Attempting uninstall: six\r\n",
      "    Found existing installation: six 1.16.0\r\n",
      "    Uninstalling six-1.16.0:\r\n",
      "      Successfully uninstalled six-1.16.0\r\n",
      "  Attempting uninstall: setuptools\r\n",
      "    Found existing installation: setuptools 63.4.1\r\n",
      "    Uninstalling setuptools-63.4.1:\r\n",
      "      Successfully uninstalled setuptools-63.4.1\r\n",
      "  Attempting uninstall: rsa\r\n",
      "    Found existing installation: rsa 4.9\r\n",
      "    Uninstalling rsa-4.9:\r\n",
      "      Successfully uninstalled rsa-4.9\r\n",
      "  Attempting uninstall: pyparsing\r\n",
      "    Found existing installation: pyparsing 3.0.9\r\n",
      "    Uninstalling pyparsing-3.0.9:\r\n",
      "      Successfully uninstalled pyparsing-3.0.9\r\n",
      "  Attempting uninstall: pyasn1-modules\r\n",
      "    Found existing installation: pyasn1-modules 0.2.8\r\n",
      "    Uninstalling pyasn1-modules-0.2.8:\r\n",
      "      Successfully uninstalled pyasn1-modules-0.2.8\r\n",
      "  Attempting uninstall: protobuf\r\n",
      "    Found existing installation: protobuf 3.19.4\r\n",
      "    Uninstalling protobuf-3.19.4:\r\n",
      "      Successfully uninstalled protobuf-3.19.4\r\n",
      "  Attempting uninstall: oauthlib\r\n",
      "    Found existing installation: oauthlib 3.2.0\r\n",
      "    Uninstalling oauthlib-3.2.0:\r\n",
      "      Successfully uninstalled oauthlib-3.2.0\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.23.2\r\n",
      "    Uninstalling numpy-1.23.2:\r\n",
      "      Successfully uninstalled numpy-1.23.2\r\n",
      "  Attempting uninstall: MarkupSafe\r\n",
      "    Found existing installation: MarkupSafe 2.1.1\r\n",
      "    Uninstalling MarkupSafe-2.1.1:\r\n",
      "      Successfully uninstalled MarkupSafe-2.1.1\r\n",
      "  Attempting uninstall: idna\r\n",
      "    Found existing installation: idna 3.3\r\n",
      "    Uninstalling idna-3.3:\r\n",
      "      Successfully uninstalled idna-3.3\r\n",
      "  Attempting uninstall: gast\r\n",
      "    Found existing installation: gast 0.4.0\r\n",
      "    Uninstalling gast-0.4.0:\r\n",
      "      Successfully uninstalled gast-0.4.0\r\n",
      "  Attempting uninstall: charset-normalizer\r\n",
      "    Found existing installation: charset-normalizer 2.0.4\r\n",
      "    Uninstalling charset-normalizer-2.0.4:\r\n",
      "      Successfully uninstalled charset-normalizer-2.0.4\r\n",
      "  Attempting uninstall: certifi\r\n",
      "    Found existing installation: certifi 2022.6.15\r\n",
      "    Uninstalling certifi-2022.6.15:\r\n",
      "      Successfully uninstalled certifi-2022.6.15\r\n",
      "  Attempting uninstall: cachetools\r\n",
      "    Found existing installation: cachetools 5.2.0\r\n",
      "    Uninstalling cachetools-5.2.0:\r\n",
      "      Successfully uninstalled cachetools-5.2.0\r\n",
      "  Attempting uninstall: absl-py\r\n",
      "    Found existing installation: absl-py 1.2.0\r\n",
      "    Uninstalling absl-py-1.2.0:\r\n",
      "      Successfully uninstalled absl-py-1.2.0\r\n",
      "  Attempting uninstall: werkzeug\r\n",
      "    Found existing installation: Werkzeug 2.2.2\r\n",
      "    Uninstalling Werkzeug-2.2.2:\r\n",
      "      Successfully uninstalled Werkzeug-2.2.2\r\n",
      "  Attempting uninstall: requests\r\n",
      "    Found existing installation: requests 2.28.1\r\n",
      "    Uninstalling requests-2.28.1:\r\n",
      "      Successfully uninstalled requests-2.28.1\r\n",
      "  Attempting uninstall: packaging\r\n",
      "    Found existing installation: packaging 21.3\r\n",
      "    Uninstalling packaging-21.3:\r\n",
      "      Successfully uninstalled packaging-21.3\r\n",
      "  Attempting uninstall: opt-einsum\r\n",
      "    Found existing installation: opt-einsum 3.3.0\r\n",
      "    Uninstalling opt-einsum-3.3.0:\r\n",
      "      Successfully uninstalled opt-einsum-3.3.0\r\n",
      "  Attempting uninstall: keras-preprocessing\r\n",
      "    Found existing installation: Keras-Preprocessing 1.1.2\r\n",
      "    Uninstalling Keras-Preprocessing-1.1.2:\r\n",
      "      Successfully uninstalled Keras-Preprocessing-1.1.2\r\n",
      "  Attempting uninstall: importlib-metadata\r\n",
      "    Found existing installation: importlib-metadata 4.12.0\r\n",
      "    Uninstalling importlib-metadata-4.12.0:\r\n",
      "      Successfully uninstalled importlib-metadata-4.12.0\r\n",
      "  Attempting uninstall: h5py\r\n",
      "    Found existing installation: h5py 3.7.0\r\n",
      "    Uninstalling h5py-3.7.0:\r\n",
      "      Successfully uninstalled h5py-3.7.0\r\n",
      "  Attempting uninstall: grpcio\r\n",
      "    Found existing installation: grpcio 1.48.1\r\n",
      "    Uninstalling grpcio-1.48.1:\r\n",
      "      Successfully uninstalled grpcio-1.48.1\r\n",
      "  Attempting uninstall: google-pasta\r\n",
      "    Found existing installation: google-pasta 0.2.0\r\n",
      "    Uninstalling google-pasta-0.2.0:\r\n",
      "      Successfully uninstalled google-pasta-0.2.0\r\n",
      "  Attempting uninstall: google-auth\r\n",
      "    Found existing installation: google-auth 2.11.0\r\n",
      "    Uninstalling google-auth-2.11.0:\r\n",
      "      Successfully uninstalled google-auth-2.11.0\r\n",
      "  Attempting uninstall: astunparse\r\n",
      "    Found existing installation: astunparse 1.6.3\r\n",
      "    Uninstalling astunparse-1.6.3:\r\n",
      "      Successfully uninstalled astunparse-1.6.3\r\n",
      "  Attempting uninstall: requests-oauthlib\r\n",
      "    Found existing installation: requests-oauthlib 1.3.1\r\n",
      "    Uninstalling requests-oauthlib-1.3.1:\r\n",
      "      Successfully uninstalled requests-oauthlib-1.3.1\r\n",
      "  Attempting uninstall: markdown\r\n",
      "    Found existing installation: Markdown 3.4.1\r\n",
      "    Uninstalling Markdown-3.4.1:\r\n",
      "      Successfully uninstalled Markdown-3.4.1\r\n",
      "  Attempting uninstall: google-auth-oauthlib\r\n",
      "    Found existing installation: google-auth-oauthlib 0.4.6\r\n",
      "    Uninstalling google-auth-oauthlib-0.4.6:\r\n",
      "      Successfully uninstalled google-auth-oauthlib-0.4.6\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.9.1\r\n",
      "    Uninstalling tensorboard-2.9.1:\r\n",
      "      Successfully uninstalled tensorboard-2.9.1\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.9.2\r\n",
      "    Uninstalling tensorflow-2.9.2:\r\n",
      "      Successfully uninstalled tensorflow-2.9.2\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "thinc 8.0.15 requires pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4, but you have pydantic 1.9.0 which is incompatible.\r\n",
      "spacy 3.3.1 requires pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4, but you have pydantic 1.9.0 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed MarkupSafe-2.1.1 absl-py-1.2.0 astunparse-1.6.3 cachetools-5.2.0 certifi-2022.6.15.1 charset-normalizer-2.1.1 flatbuffers-2.0.7 gast-0.4.0 google-auth-2.11.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.48.1 h5py-3.7.0 idna-3.3 importlib-metadata-4.12.0 keras-2.10.0 keras-preprocessing-1.1.2 libclang-14.0.6 markdown-3.4.1 numpy-1.23.2 oauthlib-3.2.0 opt-einsum-3.3.0 packaging-21.3 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 pyparsing-3.0.9 requests-2.28.1 requests-oauthlib-1.3.1 rsa-4.9 setuptools-65.3.0 six-1.16.0 tensorboard-2.10.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.27.0 termcolor-1.1.0 typing-extensions-4.3.0 urllib3-1.26.12 werkzeug-2.2.2 wheel-0.37.1 wrapt-1.14.1 zipp-3.8.1\r\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install --force-reinstall tensorflow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Data Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x7fdf943441c0>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAGsCAYAAABAeaTxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYNklEQVR4nO3df5DVdf3o8ddZF1gM/QrCF9Ovk6OIZKhQm8rAhJFok3pNAs3Uklv+CMLkmyuOxjR3sAa+mabXorTy2xXC/ImGOuq9plIi6qRYfi+FTsokP8RFkE1YYPd9/1B22PQmKOyB83o8ZpjZ/ZzDOa/X7OHsc8/nAJVSSgkAIK26ag8AAFSXGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkFz99ly5uXld7Ih/r7BSidh337122O3timp9x1rfL8KOtaDW94uwYy3Ymfttue33sl0xUErs0EF39O3timp9x1rfL8KOtaDW94uwYy2o5n5OEwBAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASK6+2gOweyqlxPr162PDhg1RSrWn2Xbl7WErlcp7XrdSiVi/vn632zEiokePHtu0I0CEGOB9am1tjXHj/lu1x+D/4+abb4uGhoZqjwHsJpwmAIDkvDLAB9Yy5MwodbvBQ6ltU+y16JaIiFh31Bcj9uhW5YF2rEr75uj17JxqjwHshnaDZ3B2daWufvf7xrpHt91v5vewm72tAdiFOE0AAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJBc1WOglBKllGqPAcA/8PycR1VjoJQSU6dOialTp3jAAexCPD/nUl/NO29tbY0///n/dnzc0NBQzXEAeJvn51yqfpoAAKguMQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAIBtdvXVM2LcuFPi6qtndDr+9NNPxte//t/j6aef3OH3OWfOrBg1alTMmTOr436uvnpGnHHGqXHLLbPe9+3uzJm3d44zzjijqnOIAQC2yapVr8aCBb+LiIgFC34Xq1a9GhERra0b4sYbfxyvvbYqbrzxx9HaumGH3ecbb6yNO++8Ndrb2+POO2+Nn/70+njttVWxYMHvOo698cba7b7dnTnz9s5xww0/jpUrV8YNN1RvDjEAwDaZOnXKu35+1123x+uvr46IiNdfXx133XX7DrvP73//e1FKiYiIUkqsWfN6p8tLKfH9739vu293Z868O85RX5V7fRcbNlSnhnaWSiVi/fr62LBhQ7z9OK4p1apXts22/nmq9cdpre8XsfN2/MfH0COP/J9obn6t07Hm5tdi7tw7Yu7c2zt9w5479/YYOXJUfPjD+3+gGZ577tlYvPi/3vN6ixf/Vzz33LNx5JFDtul2ly9fttNm3h67yhwRVY6BstUj97zzzqniJHwgtfosu7vx54mdZPPmzfGTn/zPd71s9uz/fMexUkr8/Oc/iSuu+B9RqVTe1322t7fHNdf8xzZf/5pr/iN+/vNZUVf3z1/w3jJb+YfnrR0x8/bYVebYwmkCAP6p3/72f0dbW9s2X7+9vT0WLXomXnnlb+/7Pp955uloaVm3zddvaVkXzzzz9Hte75VX/haLFj0T7e3tnY7viJm3x64yxxZVfWVg6+q58cabo6GhoYrT7FiVSsS++/aK5uaWmvzBubV1Q3zta2//9NmF9co/8T7+PNX647TW94vYeTtu2LCh4xWmUaNGx+zZ/7nNQVBXVxdHHjkkDjjg3973/Q8d2hi9eu21zUGw1157x9Chje95vQMO+Lc46qih8cc/Lur0jXhHzLw9dpU5tthl3jPQ0NBQczHQs2fPaGjYXJNPQr7/79q29c9ThsdpLe8X0TU77rHHHnHhhZPiRz/64TsuO/vs8TFnzv/qFAqVSiW++tULP9DL3HV1dTF58qUxbdrUbbr+5MmXvucpgq1nmzx5wrse76qX5neVObZwmgCA93TccZ+Jffft2+nYvvv2jVNPHROf//zYjm9elUolPv/5sbHffh/+wPd55JFDYtCgw9/zeoMGHR5HHHHUNt/uhz+8/06beXvsKnNEiAEAttG0aTPe9fPTThsbvXv3iYiIPn36xGmnjd1h99nUdPlW3yzrYp99ene6vK6uLpqaLt/u292ZM++Oc4gBALZJv37/GsOGjYiIiGHDRkS/fv8aERE9ejTEeedNiL59+8XXvjYhevTYcad89977X2LMmNOjrq4uxowZFxdc8I3o27dfDBs2Iurq6uK008bF3nv/y3bf7s6ceXvnOP/8CdG/f/8477zqzbHLvGcAgF3fv//7lIiY8o7jjY1HR2Pj0TvlPs888+yYNOnr8dpr66KU2GH3szNn3t45PvvZz3TsVw1eGQCA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAILn6at55jx494rDDPtrxMQC7Bs/PuVQ1BiqVSkybNqPjYwB2DZ6fc6lqDER4kAHsqjw/5+E9AwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAydVXewB2f5X2zVGqPcS2aNv07h/XiEr75mqPAOymxAAfWK9n51R7hO2216Jbqj0CwC7DaQIASM4rA7wvPXr0iPvvvz+am1ui7BbnCN5S3h62Uqm853UrlYh99+212+0Y8dbXB2BbiQHel0qlEj179oyGhs273TfKbVWpRM3vCBDhNAEApCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMmJAQBITgwAQHJiAACSEwMAkJwYAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOTEAAAkV789V65UdsydbrmdHXV7u6Ja37HW94uwYy2o9f0i7FgLduZ+23qblVJK2fF3DwDsLpwmAIDkxAAAJCcGACA5MQAAyYkBAEhODABAcmIAAJITAwCQnBgAgOS6PAaam5tjwoQJ0djYGMccc0x897vfjc2bN3f1GDvF6tWrY/To0bFw4cKOY4sWLYpx48bF0KFDY9SoUXHbbbdVccL3Z/HixTF+/Pg4+uijY/jw4XHppZfG6tWrI6I29ouIWLBgQYwbNy4+/vGPx/Dhw2PatGmxYcOGiKidHSMi2tra4pxzzonLLrus41it7HfffffF4YcfHkOHDu341dTUFBG1s+OaNWvi0ksvjWOOOSY++clPxoQJE+LVV1+NiNrY8Z577un09Rs6dGgMHjw4Bg8eHBG1sePzzz8fZ511VjQ2NsaIESPiyiuvjI0bN0ZElfcrXezss88u3/rWt8qbb75Zli5dWk466aRy4403dvUYO9zTTz9djj/++DJw4MDyxBNPlFJKWbNmTTn66KPLrFmzyqZNm8rjjz9ehg4dWhYtWlTlabfd+vXry/Dhw8u1115bWltby+rVq8t5551XLrjggprYr5RSmpubyxFHHFHuuOOO0tbWVlauXFlOPvnkcu2119bMjlv88Ic/LIMGDSpTpkwppdTGY3SL6dOnl8suu+wdx2tpx7PPPrtMnDixrF27tqxbt6584xvfKOeff35N7bi1FStWlOHDh5e5c+fWxI5tbW1l+PDh5Ze//GVpa2sry5cvLyeeeGK5/vrrq75fl74y8PLLL8eTTz4ZTU1N0bNnzzjwwANjwoQJMXv27K4cY4e766674pJLLonJkyd3Ov7ggw/GPvvsE2eddVbU19fHsGHD4pRTTtmt9l22bFkMGjQoJk6cGN27d4/evXvHGWecEU899VRN7BcR0adPn3j88cdjzJgxUalUYs2aNdHa2hp9+vSpmR0j3nr148EHH4wTTjih41gt7ffHP/6x4yfIrdXKjn/6059i0aJFMX369Nh7772jV69eMW3atLjkkktqZsetlVKiqakpjjvuuDj11FNrYse1a9fGqlWror29Pcrb/y1QXV1d9OzZs+r7dWkMLFmyJPbZZ5/o379/x7FDDjkkli1bFm+88UZXjrJDjRgxIh566KH43Oc+1+n4kiVLYuDAgZ2ODRgwIBYvXtyV430gBx98cPzsZz+LPfbYo+PYAw88EB/72MdqYr8tevXqFRERI0eOjFNOOSX69esXY8aMqZkdm5ub44orrogf/OAH0bNnz47jtbJfe3t7PP/88/HII4/Epz/96fjUpz4VU6dOjbVr19bMjs8991wMGDAgbr311hg9enSMGDEiZsyYEf369auZHbd29913xwsvvNBxSqsWduzdu3ece+65MWPGjDjiiCNi5MiRcdBBB8W5555b9f26NAb+/ve/d3oiioiOz998882uHGWH6tevX9TXv/N/g363fRsaGnbbXUspcc0118Rvf/vbuOKKK2puv4i3fop87LHHoq6uLi666KKa2LG9vT2amppi/PjxMWjQoE6X1cJ+EW+9X+fwww+PE088Me6777645ZZb4qWXXoqmpqaa2XHt2rXx5z//OV566aW46667Yu7cubFy5cqYMmVKzey4RXt7e8ycOTMuvPDCjlCvhR3b29ujoaEhpk6dGs8++2zMmzcvXnzxxbjuuuuqvl+XxsCee+4Z69ev73Rsy+cf+tCHunKULtGzZ8+ON6FtsWHDht1y15aWlrjoooviN7/5TcyaNSsOO+ywmtpvi4aGhujfv380NTXF/Pnza2LHn/70p9G9e/c455xz3nFZLewXEdG3b9+YPXt2jB07Nnr27Bn7779/NDU1xWOPPRallJrYsXv37hERccUVV0SvXr2ib9++cfHFF8ejjz5aMztusXDhwnj11Vdj7NixHcdq4bH60EMPxQMPPBBf+tKXonv37nHooYfGxIkTY86cOVXfr0tj4NBDD401a9bEa6+91nHsxRdfjP322y/22muvrhylSwwcODCWLFnS6dgLL7wQhx56aJUmen+WLl0aX/jCF6KlpSVuv/32OOywwyKidvb7wx/+EJ/97Gc73tEbEbFx48bo1q1bDBgwYLff8e67744nn3wyGhsbo7GxMebNmxfz5s2LxsbGmvkaLl68OK666qqO87ARb30N6+rq4sgjj6yJHQcMGBDt7e2xadOmjmPt7e0REfHRj360Jnbc4oEHHojRo0fHnnvu2XGsFh6ry5cv7/Q8ExFRX18f3bp1q/5+XfI2xa2ceeaZZfLkyWXdunUdf5vguuuu6+oxdpqt/zbB6tWrS2NjY7npppvKxo0by4IFC8rQoUPLggULqjzltluzZk057rjjymWXXVba2to6XVYL+5VSSktLSxk5cmT53ve+V1pbW8vf/va3Mnbs2PKd73ynZnbc2pQpUzr+NkGt7Ld8+fIyZMiQcsMNN5RNmzaVV155pZx++unl8ssvr5kdN27cWEaPHl0mTZpUWlpaSnNzc/nyl79cJk6cWDM7bnHyySeXW2+9tdOxWthxyZIlZfDgwWXmzJll8+bNZenSpeXkk08u06dPr/p+XR4Dq1atKpMmTSpHH310OfbYY8v06dPL5s2bu3qMnWbrGCillOeee66cccYZZejQoeUzn/lMueOOO6o43fb7xS9+UQYOHFiOOuqoMmTIkE6/Stn999tiyZIlZfz48aWxsbF8+tOfLldffXVpbW0tpdTOjltsHQOl1M5+Cxcu7Njj2GOPLdOmTSsbNmwopdTOjitWrCgXX3xxGT58eGlsbCyXXnppWbt2bSmldnYspZQhQ4aURx555B3Ha2HH3//+92XcuHHlE5/4RDnuuON2meeaSilbva4GAKTjnyMGgOTEAAAkJwYAIDkxAADJiQEASE4MAEByYgAAkhMDAJCcGACA5MQAACRXX+0BgJ3n4YcfjhtuuCFefvnlePPNN+OII46IK6+8Mg466KC4995747rrrovm5uY46qijYv/9949NmzbF9OnTo5QSN998c8yePTuam5tj4MCBcfnll8fgwYOrvRKwE3hlAGrUihUr4pvf/Gacf/75sWDBgnjkkUeilBI/+tGP4plnnokpU6bElClT4oknnogvfvGLceedd3b83l/96ldx0003xbXXXhsLFiyIMWPGxPjx4zv99+NA7RADUKP69OkT9957b4waNSpaWlpixYoV0bt371i5cmXccccdccIJJ8SoUaOivr4+Ro8eHccff3zH7509e3ZccMEFMWjQoOjWrVuMHTs2DjnkkLjnnnuquBGwszhNADWqW7duMW/evLjllluiUqnEwIEDo6WlJerr62P58uVx+OGHd7r+gQce2PGT/yuvvBIzZsyIq666quPyzZs3O00ANUoMQI26//77Y9asWTFnzpz4yEc+EhER06ZNi7/85S9xwAEHxLJlyzpdf9myZdG9e/eIiNhvv/3ioosuipNOOqnj8qVLl8Y+++zTdQsAXcZpAqhR69ati7q6umhoaIhSSjz22GMxd+7c2LRpU4wbNy4eeuihmD9/frS1tcWjjz4aDz74YMfvPf3002PmzJnx4osvRkTE/Pnz46STToqnnnqqWusAO1GllFKqPQSw423cuDG+/e1vx8MPPxx77LFHHHzwwTFs2LCYPXt2zJ8/P+699964/vrr4/XXX4/GxsYopcR+++0X06ZNi7a2trjpppvitttui1dffTX69+8fX/3qV2PcuHHVXgvYCcQAJPTXv/412tvb45BDDuk4NmnSpDj44INj8uTJVZwMqAanCSChF154Ib7yla/E0qVLIyJi4cKFMX/+/Bg5cmSVJwOqwSsDkNTMmTPj17/+daxduzYOOOCAuOCCC+KUU06p9lhAFYgBAEjOaQIASE4MAEByYgAAkhMDAJCcGACA5MQAACQnBgAgOTEAAMn9P3/yTvZ+OHk8AAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = sns.load_dataset(\"titanic\")\n",
    "sns.boxplot(x=df[\"age\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "     survived  pclass     sex   age  sibsp  parch     fare embarked   class  \\\n0           0       3    male  22.0      1      0   7.2500        S   Third   \n1           1       1  female  38.0      1      0  71.2833        C   First   \n2           1       3  female  26.0      0      0   7.9250        S   Third   \n3           1       1  female  35.0      1      0  53.1000        S   First   \n4           0       3    male  35.0      0      0   8.0500        S   Third   \n..        ...     ...     ...   ...    ...    ...      ...      ...     ...   \n886         0       2    male  27.0      0      0  13.0000        S  Second   \n887         1       1  female  19.0      0      0  30.0000        S   First   \n888         0       3  female   NaN      1      2  23.4500        S   Third   \n889         1       1    male  26.0      0      0  30.0000        C   First   \n890         0       3    male  32.0      0      0   7.7500        Q   Third   \n\n       who  adult_male deck  embark_town alive  alone  \n0      man        True  NaN  Southampton    no  False  \n1    woman       False    C    Cherbourg   yes  False  \n2    woman       False  NaN  Southampton   yes   True  \n3    woman       False    C  Southampton   yes  False  \n4      man        True  NaN  Southampton    no   True  \n..     ...         ...  ...          ...   ...    ...  \n886    man        True  NaN  Southampton    no   True  \n887  woman       False    B  Southampton   yes   True  \n888  woman       False  NaN  Southampton    no  False  \n889    man        True    C    Cherbourg   yes   True  \n890    man        True  NaN   Queenstown    no   True  \n\n[891 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>survived</th>\n      <th>pclass</th>\n      <th>sex</th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>fare</th>\n      <th>embarked</th>\n      <th>class</th>\n      <th>who</th>\n      <th>adult_male</th>\n      <th>deck</th>\n      <th>embark_town</th>\n      <th>alive</th>\n      <th>alone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>C</td>\n      <td>First</td>\n      <td>woman</td>\n      <td>False</td>\n      <td>C</td>\n      <td>Cherbourg</td>\n      <td>yes</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>3</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>woman</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>yes</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>S</td>\n      <td>First</td>\n      <td>woman</td>\n      <td>False</td>\n      <td>C</td>\n      <td>Southampton</td>\n      <td>yes</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>886</th>\n      <td>0</td>\n      <td>2</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13.0000</td>\n      <td>S</td>\n      <td>Second</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>1</td>\n      <td>1</td>\n      <td>female</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>S</td>\n      <td>First</td>\n      <td>woman</td>\n      <td>False</td>\n      <td>B</td>\n      <td>Southampton</td>\n      <td>yes</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>0</td>\n      <td>3</td>\n      <td>female</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>2</td>\n      <td>23.4500</td>\n      <td>S</td>\n      <td>Third</td>\n      <td>woman</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>Southampton</td>\n      <td>no</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>889</th>\n      <td>1</td>\n      <td>1</td>\n      <td>male</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>C</td>\n      <td>First</td>\n      <td>man</td>\n      <td>True</td>\n      <td>C</td>\n      <td>Cherbourg</td>\n      <td>yes</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>890</th>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>32.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.7500</td>\n      <td>Q</td>\n      <td>Third</td>\n      <td>man</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>Queenstown</td>\n      <td>no</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>891 rows × 15 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x7fdf95ece7c0>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(data=df, x=\"age\", y=\"class\", hue=\"alive\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "flights = sns.load_dataset(\"flights\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "     year month  passengers\n0    1949   Jan         112\n1    1949   Feb         118\n2    1949   Mar         132\n3    1949   Apr         129\n4    1949   May         121\n..    ...   ...         ...\n139  1960   Aug         606\n140  1960   Sep         508\n141  1960   Oct         461\n142  1960   Nov         390\n143  1960   Dec         432\n\n[144 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>month</th>\n      <th>passengers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1949</td>\n      <td>Jan</td>\n      <td>112</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1949</td>\n      <td>Feb</td>\n      <td>118</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1949</td>\n      <td>Mar</td>\n      <td>132</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1949</td>\n      <td>Apr</td>\n      <td>129</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1949</td>\n      <td>May</td>\n      <td>121</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>139</th>\n      <td>1960</td>\n      <td>Aug</td>\n      <td>606</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>1960</td>\n      <td>Sep</td>\n      <td>508</td>\n    </tr>\n    <tr>\n      <th>141</th>\n      <td>1960</td>\n      <td>Oct</td>\n      <td>461</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>1960</td>\n      <td>Nov</td>\n      <td>390</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>1960</td>\n      <td>Dec</td>\n      <td>432</td>\n    </tr>\n  </tbody>\n</table>\n<p>144 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
